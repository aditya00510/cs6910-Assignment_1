{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Queston2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOgQtCy2t1SOL8y3x9ISB/h"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Hc2mEkZU5Gvl","executionInfo":{"status":"ok","timestamp":1615527838371,"user_tz":-330,"elapsed":2138,"user":{"displayName":"Varun Sai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjdid1-5pVeGn8rfYxKCP45_oK1cFmxCnNxGfRuiA=s64","userId":"11903530869297881570"}}},"source":["from keras.datasets import fashion_mnist\r\n","from matplotlib import pyplot as plt\r\n","import numpy as np\r\n","import pandas as pd\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()\r\n","x_train = x_train/255\r\n","x_test = x_test/255\r\n","\r\n","\r\n","\r\n","\r\n","class Neural_network:\r\n","    np.random.seed(10)\r\n","    def __init__(self,x_train,y_train,input_dim,hidden_layers_size,hidden_layers,output_dim,batch_size=30,epochs=10,activation_func=\"relu\"\r\n","           ,learning_rate=6e-3 ,decay_rate=0.9,beta=0.9,beta1=0.9,beta2=0.99,optimizer=\"adam\",weight_init=\"xavier\"):\r\n","\r\n","        x_train,self.x_cv,y_train,self.y_cv = train_test_split(x_train, y_train, test_size=0.10, random_state=100,stratify=y_train)\r\n","\r\n","        np.random.seed(10)\r\n","        self.input_dim = input_dim\r\n","        self.hidden_layers = hidden_layers\r\n","        self.hidden_layers_size = hidden_layers_size\r\n","        self.output_dim = output_dim\r\n","\r\n","        self.batch = batch_size\r\n","        self.epochs = epochs\r\n","        self.activation_func = activation_func\r\n","        self.learning_rate = learning_rate\r\n","        self.decay_rate = decay_rate\r\n","        self.optimizer = optimizer\r\n","        self.weight_init = weight_init\r\n","        self.beta = beta\r\n","        self.beta1 = beta1\r\n","        self.beta2 = beta2\r\n","\r\n","\r\n","        self.layers = [self.input_dim] + self.hidden_layers*[self.hidden_layers_size] + [self.output_dim]\r\n","\r\n","        layers = self.layers.copy()\r\n","        self.weights = []\r\n","        self.biases = []\r\n","        self.activations = []\r\n","        self.activation_gradients = []\r\n","        self.weights_gradients = []\r\n","        self.biases_gradients = []\r\n","\r\n","        for i in range(len(layers)-1):\r\n","            if self.weight_init == 'xavier':\r\n","                std = np.sqrt(2/(layers[i]*layers[i+1]))\r\n","                self.weights.append(np.random.normal(0,std,(layers[i],layers[i+1])))\r\n","                self.biases.append(np.random.normal(0,std,(layers[i+1])))\r\n","            else:\r\n","                self.weights.append(np.random.normal(0,0.5,(layers[i],layers[i+1])))\r\n","                self.biases.append(np.random.normal(0,0.5,(layers[i+1])))\r\n","            self.activations.append(np.zeros(layers[i]))\r\n","            self.activation_gradients.append(np.zeros(layers[i+1]))\r\n","            self.weights_gradients.append(np.zeros((layers[i],layers[i+1])))\r\n","            self.biases_gradients.append(np.zeros(layers[i+1]))\r\n","\r\n","        self.activations.append(np.zeros(layers[-1]))\r\n","        \r\n","        self.gradient_descent(x_train,y_train)\r\n","\r\n","\r\n","    def sigmoid(self,activations):\r\n","        res = []\r\n","        for z in activations:\r\n","            if z>40:\r\n","                res.append(1.0)\r\n","            elif z<-40:\r\n","                res.append(0.0)\r\n","            else:\r\n","                res.append(1/(1+np.exp(-z)))\r\n","        return np.asarray(res)\r\n","\r\n","    def tanh(self,activations):\r\n","        res = []\r\n","        for z in activations:\r\n","            if z>20:\r\n","                res.append(1.0)\r\n","            elif z<-20:\r\n","                res.append(-1.0)\r\n","            else:\r\n","                res.append((np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z)))\r\n","        return np.asarray(res)\r\n","\r\n","    def relu(self,activations):\r\n","        res = []\r\n","        for i in activations:\r\n","            if i<= 0:\r\n","                res.append(0)\r\n","            else:\r\n","                res.append(i)\r\n","        return np.asarray(res)\r\n","\r\n","    def softmax(self,activations):\r\n","        tot = 0\r\n","        for z in activations:\r\n","            tot += np.exp(z)\r\n","        return np.asarray([np.exp(z)/tot for z in activations])\r\n","\r\n","    def forward_propagation(self,x,y,weights,biases):\r\n","        self.activations[0] = x\r\n","        n = len(self.layers)\r\n","        for i in range(n-2):\r\n","            if self.activation_func == \"sigmoid\":\r\n","                self.activations[i+1] = self.sigmoid(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n","            elif self.activation_func == \"tanh\":\r\n","                self.activations[i+1] = self.tanh(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n","            elif self.activation_func == \"relu\":\r\n","                self.activations[i+1] = self.relu(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n","\r\n","        self.activations[n-1] = self.softmax(np.matmul(weights[n-2].T,self.activations[n-2])+biases[n-2])        \r\n","        return -(np.log2(self.activations[-1][y])) #Return cross entropy loss for single data point.\r\n","\r\n","\r\n","    def grad_w(self,i):\r\n","        return np.matmul(self.activations[i].reshape((-1,1)),self.activation_gradients[i].reshape((1,-1)))\r\n","\r\n","\r\n","    def grad_b(self,i):\r\n","        return self.activation_gradients[i]\r\n","\r\n","\r\n","    def backward_propagation(self,x,y,weights,biases):\r\n","        y_onehot = np.zeros(self.output_dim)\r\n","        y_onehot[y] = 1\r\n","        n = len(self.layers)\r\n","\r\n","        self.activation_gradients[-1] =  -1*(y_onehot - self.activations[-1])\r\n","        for i in range(n-2,-1,-1):\r\n","            self.weights_gradients[i] += self.grad_w(i)\r\n","            self.biases_gradients[i] += self.grad_b(i)\r\n","            if i!=0:\r\n","                value = np.matmul(weights[i],self.activation_gradients[i])\r\n","                if self.activation_func == \"sigmoid\":\r\n","                    self.activation_gradients[i-1] = value * self.activations[i] * (1-self.activations[i])\r\n","                elif self.activation_func == \"tanh\":\r\n","                    self.activation_gradients[i-1] = value * (1-np.square(self.activations[i]))\r\n","                elif self.activation_func == \"relu\":\r\n","                    res = []\r\n","                    for k in self.activations[i]:\r\n","                        if k>0: res.append(1.0)\r\n","                        else: res.append(0.0)\r\n","                    res = np.asarray(res)\r\n","                    self.activation_gradients[i-1] = value * res\r\n","\r\n","    def gradient_descent(self,x_train,y_train):\r\n","        for i in range(self.epochs):\r\n","            print('Epoch---',i+1,end=\" \")\r\n","            loss = 0\r\n","            val_loss = 0\r\n","\r\n","            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","            \r\n","            index = 1\r\n","            for x,y in zip(x_train,y_train):\r\n","                x = x.ravel()\r\n","                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n","                self.backward_propagation(x,y,self.weights,self.biases)\r\n","\r\n","                if index % self.batch == 0 or index == x_train.shape[0]:\r\n","                    for j in range(len(self.weights)):\r\n","                        self.weights[j] -= self.learning_rate * self.weights_gradients[j]\r\n","                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\r\n","                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","                index += 1 \r\n","              \r\n","            for x,y in zip(self.x_cv,self.y_cv):\r\n","               x=x.ravel()\r\n","               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n","\r\n","            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n","            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n","            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n","\r\n","    def calculate_accuracy(self,X,Y):\r\n","        count = 0\r\n","        for i in range(len(X)):\r\n","            if self.predict(X[i]) == Y[i]:\r\n","                count+=1\r\n","        return count/len(X)\r\n","\r\n","\r\n","    def predict(self,x):\r\n","        x = x.ravel()\r\n","        self.activations[0] = x\r\n","        n = len(self.layers)\r\n","        for i in range(n-2):\r\n","            if self.activation_func == \"sigmoid\":\r\n","                self.activations[i+1] = self.sigmoid(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n","            elif self.activation_func == \"tanh\":\r\n","                self.activations[i+1] = self.tanh(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n","            elif self.activation_func == \"relu\":\r\n","                self.activations[i+1] = self.relu(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n","\r\n","        self.activations[n-1] = self.softmax(np.matmul(self.weights[n-2].T,self.activations[n-2])+self.biases[n-2])\r\n","\r\n","        return np.argmax(self.activations[-1])  \r\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"dVjEdWuq8K8Y"},"source":[""],"execution_count":null,"outputs":[]}]}