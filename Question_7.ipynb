{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question 7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c3fd2f115f84c38a83cf9cfe7c0e538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_37f06681332c4a4eb8fc7f886c62c7c6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ec41ffe9618c492b934d7b270a491451",
              "IPY_MODEL_0024309d51b840cdb9d8741d796be9cc"
            ]
          }
        },
        "37f06681332c4a4eb8fc7f886c62c7c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec41ffe9618c492b934d7b270a491451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_0a53d31ea92e4a218ee599d9c3a64e30",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.01MB of 0.01MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac9adc80ab3448239af1ef9871ac4618"
          }
        },
        "0024309d51b840cdb9d8741d796be9cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ec0f3e8e48164971a755e1b946db65a5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8fe3ad9f77341dfb16483ad69616545"
          }
        },
        "0a53d31ea92e4a218ee599d9c3a64e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac9adc80ab3448239af1ef9871ac4618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec0f3e8e48164971a755e1b946db65a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8fe3ad9f77341dfb16483ad69616545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQzI9Pk-YBHJ",
        "outputId": "e785eb00-cacd-4540-a956-d50533e59bd0"
      },
      "source": [
        "from keras.datasets import fashion_mnist\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()\r\n",
        "x_train = x_train/255\r\n",
        "x_test = x_test/255"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbTbvaifYDNg"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "class Neural_network:\r\n",
        "    np.random.seed(10)\r\n",
        "    def __init__(self,x_train,y_train,input_dim,hidden_layers_size,hidden_layers,output_dim,batch_size=30,epochs=10,activation_func=\"relu\"\r\n",
        "           ,learning_rate=6e-3 ,decay_rate=0.9,beta=0.9,beta1=0.9,beta2=0.99,optimizer=\"adam\",weight_init=\"xavier\"): \r\n",
        "\r\n",
        "        self.x_train,self.x_cv,self.y_train,self.y_cv = train_test_split(x_train, y_train, test_size=0.10, random_state=100,stratify=y_train)\r\n",
        "\r\n",
        "        np.random.seed(10)\r\n",
        "        self.input_dim = input_dim\r\n",
        "        self.hidden_layers = hidden_layers\r\n",
        "        self.hidden_layers_size = hidden_layers_size\r\n",
        "        self.output_dim = output_dim\r\n",
        "\r\n",
        "        self.batch = batch_size\r\n",
        "        self.epochs = epochs\r\n",
        "        self.activation_func = activation_func\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.decay_rate = decay_rate\r\n",
        "        self.optimizer = optimizer\r\n",
        "        self.weight_init = weight_init\r\n",
        "        self.beta = beta\r\n",
        "        self.beta1 = beta1\r\n",
        "        self.beta2 = beta2\r\n",
        "        \r\n",
        "\r\n",
        "        self.layers = [self.input_dim] + self.hidden_layers*[self.hidden_layers_size] + [self.output_dim]\r\n",
        "\r\n",
        "        layers = self.layers.copy()\r\n",
        "        self.weights = []\r\n",
        "        self.biases = []\r\n",
        "        self.activations = []\r\n",
        "        self.activation_gradients = []\r\n",
        "        self.weights_gradients = []\r\n",
        "        self.biases_gradients = []\r\n",
        "        self.conf_matrix=[[0 for i in range(10) ] for i in range(10)]\r\n",
        "        self.actual_labels=['T-shirt/top(actaul)','Trouser(actaul)','Pullover(actaul)','Dress(actaul)','Coat(actaul)','Sandal(actaul)','Shirt(actaul)','Sneaker(actaul)','Bag(actaul)','Ankle boot(actaul)']\r\n",
        "        self.predicted_labels=['T-shirt/top(predicted)','Trouser(predicted)','Pullover(predicted)','Dress(predicted)','Coat(predicted)','Sandal(predicted)','Shirt(predicted)','Sneaker(predicted)','Bag(predicted)','Ankle boot(predicted)']\r\n",
        "\r\n",
        "        for i in range(len(layers)-1):\r\n",
        "            if self.weight_init == 'xavier':\r\n",
        "                std = np.sqrt(2/(layers[i]*layers[i+1]))\r\n",
        "                self.weights.append(np.random.normal(0,std,(layers[i],layers[i+1])))\r\n",
        "                self.biases.append(np.random.normal(0,std,(layers[i+1])))\r\n",
        "            else:\r\n",
        "                self.weights.append(np.random.normal(0,0.5,(layers[i],layers[i+1])))\r\n",
        "                self.biases.append(np.random.normal(0,0.5,(layers[i+1])))\r\n",
        "            self.activations.append(np.zeros(layers[i]))\r\n",
        "            self.activation_gradients.append(np.zeros(layers[i+1]))\r\n",
        "            self.weights_gradients.append(np.zeros((layers[i],layers[i+1])))\r\n",
        "            self.biases_gradients.append(np.zeros(layers[i+1]))\r\n",
        "\r\n",
        "        self.activations.append(np.zeros(layers[-1]))\r\n",
        "        \r\n",
        "        if optimizer == 'adam':\r\n",
        "            self.adam(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'sgd':\r\n",
        "            self.sgd(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'momentum':\r\n",
        "            self.momentum(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'nesterov':\r\n",
        "            self.nesterov(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'nadam':\r\n",
        "            self.nadam(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'rmsprop':\r\n",
        "            self.rmsprop(self.x_train,self.y_train)\r\n",
        "\r\n",
        "\r\n",
        "    def sigmoid(self,activations):\r\n",
        "        res = []\r\n",
        "        for z in activations:\r\n",
        "            if z>40:\r\n",
        "                res.append(1.0)\r\n",
        "            elif z<-40:\r\n",
        "                res.append(0.0)\r\n",
        "            else:\r\n",
        "                res.append(1/(1+np.exp(-z)))\r\n",
        "        return np.asarray(res)\r\n",
        "\r\n",
        "    def tanh(self,activations):\r\n",
        "        res = []\r\n",
        "        for z in activations:\r\n",
        "            if z>20:\r\n",
        "                res.append(1.0)\r\n",
        "            elif z<-20:\r\n",
        "                res.append(-1.0)\r\n",
        "            else:\r\n",
        "                res.append((np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z)))\r\n",
        "        return np.asarray(res)\r\n",
        "\r\n",
        "    def relu(self,activations):\r\n",
        "        res = []\r\n",
        "        for i in activations:\r\n",
        "            if i<= 0:\r\n",
        "                res.append(0)\r\n",
        "            else:\r\n",
        "                res.append(i)\r\n",
        "        return np.asarray(res)\r\n",
        "\r\n",
        "    def softmax(self,activations):\r\n",
        "        tot = 0\r\n",
        "        for z in activations:\r\n",
        "            tot += np.exp(z)\r\n",
        "        return np.asarray([np.exp(z)/tot for z in activations])\r\n",
        "\r\n",
        "    def forward_propagation(self,x,y,weights,biases):\r\n",
        "        self.activations[0] = x\r\n",
        "        n = len(self.layers)\r\n",
        "        for i in range(n-2):\r\n",
        "            if self.activation_func == \"sigmoid\":\r\n",
        "                self.activations[i+1] = self.sigmoid(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n",
        "            elif self.activation_func == \"tanh\":\r\n",
        "                self.activations[i+1] = self.tanh(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n",
        "            elif self.activation_func == \"relu\":\r\n",
        "                self.activations[i+1] = self.relu(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n",
        "\r\n",
        "        self.activations[n-1] = self.softmax(np.matmul(weights[n-2].T,self.activations[n-2])+biases[n-2])        \r\n",
        "        return -(np.log2(self.activations[-1][y])) #Return cross entropy loss for single data point.\r\n",
        "\r\n",
        "\r\n",
        "    def grad_w(self,i):\r\n",
        "        return np.matmul(self.activations[i].reshape((-1,1)),self.activation_gradients[i].reshape((1,-1)))\r\n",
        "\r\n",
        "\r\n",
        "    def grad_b(self,i):\r\n",
        "        return self.activation_gradients[i]\r\n",
        "\r\n",
        "\r\n",
        "    def backward_propagation(self,x,y,weights,biases):\r\n",
        "        y_onehot = np.zeros(self.output_dim)\r\n",
        "        y_onehot[y] = 1\r\n",
        "        n = len(self.layers)\r\n",
        "\r\n",
        "        self.activation_gradients[-1] =  -1*(y_onehot - self.activations[-1])\r\n",
        "        for i in range(n-2,-1,-1):\r\n",
        "            self.weights_gradients[i] += self.grad_w(i)\r\n",
        "            self.biases_gradients[i] += self.grad_b(i)\r\n",
        "            if i!=0:\r\n",
        "                value = np.matmul(weights[i],self.activation_gradients[i])\r\n",
        "                if self.activation_func == \"sigmoid\":\r\n",
        "                    self.activation_gradients[i-1] = value * self.activations[i] * (1-self.activations[i])\r\n",
        "                elif self.activation_func == \"tanh\":\r\n",
        "                    self.activation_gradients[i-1] = value * (1-np.square(self.activations[i]))\r\n",
        "                elif self.activation_func == \"relu\":\r\n",
        "                    res = []\r\n",
        "                    for k in self.activations[i]:\r\n",
        "                        if k>0: res.append(1.0)\r\n",
        "                        else: res.append(0.0)\r\n",
        "                    res = np.asarray(res)\r\n",
        "                    self.activation_gradients[i-1] = value * res\r\n",
        "\r\n",
        "\r\n",
        "    def gradient_descent(self,x_train,y_train):\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "            \r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        self.weights[j] -= self.learning_rate * self.weights_gradients[j]\r\n",
        "                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                index += 1 \r\n",
        "\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',round(self.calculate_accuracy(x_train,y_train),3))\r\n",
        "\r\n",
        "\r\n",
        "    def sgd(self,x_train,y_train):\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        self.weights[j] -= self.learning_rate * self.weights_gradients[j]\r\n",
        "                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                index += 1
        "\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "    \r\n",
        "    def momentum(self,x_train,y_train):\r\n",
        "        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        v_w = (self.decay_rate * prev_gradients_w[j] + self.learning_rate * self.weights_gradients[j])\r\n",
        "                        v_b = (self.decay_rate * prev_gradients_b[j] + self.learning_rate * self.biases_gradients[j])\r\n",
        "                        self.weights[j] -= v_w\r\n",
        "                        self.biases[j] -= v_b\r\n",
        "                        prev_gradients_w[j] = v_w\r\n",
        "                        prev_gradients_b[j] = v_b\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "                index +=1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def nesterov(self,x_train,y_train):\r\n",
        "        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            weights = [self.weights[j] -  self.decay_rate * prev_gradients_w[j] for j in range(len(self.weights))]\r\n",
        "            biases = [self.biases[j] -  self.decay_rate * prev_gradients_b[j] for j in range(len(self.biases))]\r\n",
        "\r\n",
        "            self.weights_gradients = [0*j for j in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*j for j in (self.biases_gradients)]\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,weights,biases)\r\n",
        "                self.backward_propagation(x,y,weights,biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        prev_gradients_w[j] = self.decay_rate * prev_gradients_w[j] + self.learning_rate*self.weights_gradients[j] \r\n",
        "                                           \r\n",
        "                        prev_gradients_b[j] = self.decay_rate * prev_gradients_b[j] + self.learning_rate*self.biases_gradients[j] \r\n",
        "                                        \r\n",
        "                        self.weights[j] -= prev_gradients_w[j]\r\n",
        "                        self.biases[j] -= prev_gradients_b[j]\r\n",
        "\r\n",
        "                    weights = [self.weights[j] -  self.decay_rate * prev_gradients_w[j] for j in range(len(self.weights))]\r\n",
        "                    biases = [self.biases[j] -  self.decay_rate * prev_gradients_b[j] for j in range(len(self.biases))]\r\n",
        "\r\n",
        "                    self.weights_gradients = [0*j for j in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*j for j in (self.biases_gradients)]\r\n",
        "                    \r\n",
        "                index += 1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def rmsprop(self,x_train,y_train):\r\n",
        "        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "        eps = 1e-2\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        v_w = (self.beta * prev_gradients_w[j] + (1-self.beta) * np.square(self.weights_gradients[j]))\r\n",
        "                        v_b = (self.beta * prev_gradients_b[j] + (1-self.beta) * np.square(self.biases_gradients[j]))\r\n",
        "                        self.weights[j] -= self.learning_rate * (self.weights_gradients[j] /(np.sqrt(v_w + eps)))\r\n",
        "                        self.biases[j] -= self.learning_rate * (self.biases_gradients[j] /(np.sqrt(v_b + eps)))\r\n",
        "                        prev_gradients_w[j] = v_w\r\n",
        "                        prev_gradients_b[j] = v_b\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "                index +=1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def adam(self,x_train,y_train):\r\n",
        "        m_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        m_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "        v_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        v_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        iter = 1\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "            eps = 1e-2\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j] + (1-self.beta1) * self.weights_gradients[j])\r\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j] + (1-self.beta1) * self.biases_gradients[j])\r\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j] + (1-self.beta2) * np.square(self.weights_gradients[j]))\r\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j] + (1-self.beta2) * np.square(self.biases_gradients[j]))\r\n",
        "\r\n",
        "                        m_hat_w = (m_w)/(1-(self.beta1)**iter) \r\n",
        "                        m_hat_b = (m_b)/(1-(self.beta1)**iter) \r\n",
        "\r\n",
        "                        v_hat_w = (v_w)/(1-(self.beta2)**iter) \r\n",
        "                        v_hat_b = (v_b)/(1-(self.beta2)**iter)\r\n",
        "\r\n",
        "                        self.weights[j] -= self.learning_rate * (m_hat_w/(np.sqrt(v_hat_w + eps)))\r\n",
        "                        self.biases[j] -= self.learning_rate * (m_hat_b/(np.sqrt(v_hat_b + eps)))\r\n",
        "\r\n",
        "                        m_prev_gradients_w[j] = m_w\r\n",
        "                        m_prev_gradients_b[j] = m_b\r\n",
        "                        v_prev_gradients_w[j] = v_w\r\n",
        "                        v_prev_gradients_b[j] = v_b\r\n",
        "\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                    iter += 1\r\n",
        "\r\n",
        "                index +=1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "    def nadam(self,x_train,y_train):\r\n",
        "        m_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        m_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "        v_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        v_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        iter = 1\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "            eps = 1e-2\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "\r\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j] + (1-self.beta1) * self.weights_gradients[j])\r\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j] + (1-self.beta1) * self.biases_gradients[j])\r\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j] + (1-self.beta2) * np.square(self.weights_gradients[j]))\r\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j] + (1-self.beta2) * np.square(self.biases_gradients[j]))\r\n",
        "\r\n",
        "                        m_hat_w = (m_w)/(1-(self.beta1)**iter) \r\n",
        "                        m_hat_b = (m_b)/(1-(self.beta1)**iter) \r\n",
        "\r\n",
        "                        v_hat_w = (v_w)/(1-(self.beta2)**iter) \r\n",
        "                        v_hat_b = (v_b)/(1-(self.beta2)**iter)\r\n",
        "\r\n",
        "                        m_dash_w = self.beta1 * m_hat_w + (1-self.beta1) * self.weights_gradients[j]\r\n",
        "                        m_dash_b = self.beta1 * m_hat_b + (1-self.beta1) * self.biases_gradients[j]\r\n",
        "\r\n",
        "                        self.weights[j] -= self.learning_rate * (m_dash_w/(np.sqrt(v_hat_w + eps)))\r\n",
        "                        self.biases[j] -= self.learning_rate * (m_dash_b/(np.sqrt(v_hat_b + eps)))\r\n",
        "\r\n",
        "                        m_prev_gradients_w[j] = m_w\r\n",
        "                        m_prev_gradients_b[j] = m_b\r\n",
        "                        v_prev_gradients_w[j] = v_w\r\n",
        "                        v_prev_gradients_b[j] = v_b\r\n",
        "\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                    iter += 1\r\n",
        "\r\n",
        "                index +=1\r\n",
        "\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            #wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "    \r\n",
        "\r\n",
        "    def calculate_accuracy(self,X,Y,is_test=False):\r\n",
        "        count = 0\r\n",
        "        for i in range(len(X)):\r\n",
        "            temp=self.predict(X[i])\r\n",
        "            if temp == Y[i]:\r\n",
        "                count+=1\r\n",
        "            if is_test:\r\n",
        "              self.conf_matrix[temp][Y[i]]+=1\r\n",
        "        if is_test:\r\n",
        "          wandb.log({'Confusion matrix': wandb.plots.HeatMap(self.actual_labels, self.predicted_labels, self.conf_matrix, show_text=True)})\r\n",
        "        return count/len(X)\r\n",
        "\r\n",
        "\r\n",
        "    def predict(self,x):\r\n",
        "        x = x.ravel()\r\n",
        "        self.activations[0] = x\r\n",
        "        n = len(self.layers)\r\n",
        "        for i in range(n-2):\r\n",
        "            if self.activation_func == \"sigmoid\":\r\n",
        "                self.activations[i+1] = self.sigmoid(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n",
        "            elif self.activation_func == \"tanh\":\r\n",
        "                self.activations[i+1] = self.tanh(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n",
        "            elif self.activation_func == \"relu\":\r\n",
        "                self.activations[i+1] = self.relu(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n",
        "\r\n",
        "        self.activations[n-1] = self.softmax(np.matmul(self.weights[n-2].T,self.activations[n-2])+self.biases[n-2])\r\n",
        "\r\n",
        "        return np.argmax(self.activations[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlCM5QonYLJ5",
        "outputId": "7e09da9c-e197-4240-94d7-3eeb6ec2fb49"
      },
      "source": [
        "!pip install --upgrade wandb\r\n",
        "import wandb\r\n",
        "!wandb login 3c967c63b099a3b2acd600aa30008e7de1ea6498"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: wandb in /usr/local/lib/python3.7/dist-packages (0.10.22)\n",
            "Requirement already satisfied, skipping upgrade: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied, skipping upgrade: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied, skipping upgrade: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.5)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708,
          "referenced_widgets": [
            "4c3fd2f115f84c38a83cf9cfe7c0e538",
            "37f06681332c4a4eb8fc7f886c62c7c6",
            "ec41ffe9618c492b934d7b270a491451",
            "0024309d51b840cdb9d8741d796be9cc",
            "0a53d31ea92e4a218ee599d9c3a64e30",
            "ac9adc80ab3448239af1ef9871ac4618",
            "ec0f3e8e48164971a755e1b946db65a5",
            "c8fe3ad9f77341dfb16483ad69616545"
          ]
        },
        "id": "BRqvUbs1YWyX",
        "outputId": "6aea7d5b-69c2-4ea2-b538-4b26ecab4c53"
      },
      "source": [
        "wandb.init(project='fashion_mnist',entity='adi00510',reinit=True)\r\n",
        "nn=Neural_network(x_train,y_train,784,64,2,10,64,7,'tanh',0.002,0.1,0.9,0.9,0.99,'nadam','xavier')\r\n",
        "\r\n",
        "nn.calculate_accuracy(x_test,y_test,True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:u4klywb3) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 1030<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c3fd2f115f84c38a83cf9cfe7c0e538",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/content/wandb/run-20210311_055935-u4klywb3/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/content/wandb/run-20210311_055935-u4klywb3/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>_runtime</td><td>55</td></tr><tr><td>_timestamp</td><td>1615442434</td></tr><tr><td>_step</td><td>0</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>_runtime</td><td>▁</td></tr><tr><td>_timestamp</td><td>▁</td></tr><tr><td>_step</td><td>▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    <br/>Synced <strong style=\"color:#cdcd00\">breezy-galaxy-211</strong>: <a href=\"https://wandb.ai/adi00510/fashion_mnist/runs/u4klywb3\" target=\"_blank\">https://wandb.ai/adi00510/fashion_mnist/runs/u4klywb3</a><br/>\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:u4klywb3). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">resilient-puddle-212</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/adi00510/fashion_mnist\" target=\"_blank\">https://wandb.ai/adi00510/fashion_mnist</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/adi00510/fashion_mnist/runs/owt822yw\" target=\"_blank\">https://wandb.ai/adi00510/fashion_mnist/runs/owt822yw</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210311_060139-owt822yw</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch--- 1   loss =  0.8026560179978357   accuracy =  0.856    validation loss=  0.5974650405768595   validation accuaracy=  0.842\n",
            "Epoch--- 2   loss =  0.5472850285954772   accuracy =  0.869    validation loss=  0.5443255065183352   validation accuaracy=  0.855\n",
            "Epoch--- 3   loss =  0.4970527330993899   accuracy =  0.875    validation loss=  0.5257676483428697   validation accuaracy=  0.86\n",
            "Epoch--- 4   loss =  0.465177442865648   accuracy =  0.883    validation loss=  0.5032145444455606   validation accuaracy=  0.867\n",
            "Epoch--- 5   loss =  0.44067019984154865   accuracy =  0.887    validation loss=  0.48671335359921153   validation accuaracy=  0.875\n",
            "Epoch--- 6   loss =  0.4213724798032104   accuracy =  0.891    validation loss=  0.4873435136591763   validation accuaracy=  0.877\n",
            "Epoch--- 7   loss =  0.40495007201245414   accuracy =  0.893    validation loss=  0.4800940817641748   validation accuaracy=  0.879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Visualizing heatmap.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8643"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZwnAZffYXBp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
