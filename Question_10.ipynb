{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3rMxPSfXtRf",
        "outputId": "a7e1dbd7-7b22-48a7-bdf4-20a77309179e"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from keras.datasets import fashion_mnist\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "(x_train,y_train),(x_test,y_test) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\r\n",
        "x_train = x_train/255\r\n",
        "x_test = x_test/255"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2TAWZFhXnrP"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "class Neural_network:\r\n",
        "    np.random.seed(10)\r\n",
        "    def __init__(self,x_train,y_train,input_dim,hidden_layers_size,hidden_layers,output_dim,batch_size=30,epochs=10,activation_func=\"relu\"\r\n",
        "           ,learning_rate=6e-3 ,decay_rate=0.9,beta=0.9,beta1=0.9,beta2=0.99,optimizer=\"adam\",weight_init=\"xavier\",loss=\"cross_entropy\"):\r\n",
        "\r\n",
        "        self.x_train,self.x_cv,self.y_train,self.y_cv = train_test_split(x_train, y_train, test_size=0.10, random_state=100,stratify=y_train)\r\n",
        "\r\n",
        "        np.random.seed(10)\r\n",
        "        self.input_dim = input_dim\r\n",
        "        self.hidden_layers = hidden_layers\r\n",
        "        self.hidden_layers_size = hidden_layers_size\r\n",
        "        self.output_dim = output_dim\r\n",
        "\r\n",
        "        self.batch = batch_size\r\n",
        "        self.epochs = epochs\r\n",
        "        self.activation_func = activation_func\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.decay_rate = decay_rate\r\n",
        "        self.optimizer = optimizer\r\n",
        "        self.weight_init = weight_init\r\n",
        "        self.beta = beta\r\n",
        "        self.beta1 = beta1\r\n",
        "        self.beta2 = beta2\r\n",
        "        self.loss = loss\r\n",
        "\r\n",
        "        self.layers = [self.input_dim] + self.hidden_layers*[self.hidden_layers_size] + [self.output_dim]\r\n",
        "\r\n",
        "        layers = self.layers.copy()\r\n",
        "        self.weights = []\r\n",
        "        self.biases = []\r\n",
        "        self.activations = []\r\n",
        "        self.activation_gradients = []\r\n",
        "        self.weights_gradients = []\r\n",
        "        self.biases_gradients = []\r\n",
        "\r\n",
        "        for i in range(len(layers)-1):\r\n",
        "            if self.weight_init == 'xavier':\r\n",
        "                std = np.sqrt(2/(layers[i]*layers[i+1]))\r\n",
        "                self.weights.append(np.random.normal(0,std,(layers[i],layers[i+1])))\r\n",
        "                self.biases.append(np.random.normal(0,std,(layers[i+1])))\r\n",
        "            else:\r\n",
        "                self.weights.append(np.random.normal(0,0.5,(layers[i],layers[i+1])))\r\n",
        "                self.biases.append(np.random.normal(0,0.5,(layers[i+1])))\r\n",
        "            self.activations.append(np.zeros(layers[i]))\r\n",
        "            self.activation_gradients.append(np.zeros(layers[i+1]))\r\n",
        "            self.weights_gradients.append(np.zeros((layers[i],layers[i+1])))\r\n",
        "            self.biases_gradients.append(np.zeros(layers[i+1]))\r\n",
        "\r\n",
        "        self.activations.append(np.zeros(layers[-1]))\r\n",
        "        \r\n",
        "        if optimizer == 'adam':\r\n",
        "            self.adam(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'sgd':\r\n",
        "            self.sgd(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'momentum':\r\n",
        "            self.momentum(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'nesterov':\r\n",
        "            self.nesterov(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'nadam':\r\n",
        "            self.nadam(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'rmsprop':\r\n",
        "            self.rmsprop(self.x_train,self.y_train)\r\n",
        "\r\n",
        "\r\n",
        "    def sigmoid(self,activations):\r\n",
        "        res = []\r\n",
        "        for z in activations:\r\n",
        "            if z>40:\r\n",
        "                res.append(1.0)\r\n",
        "            elif z<-40:\r\n",
        "                res.append(0.0)\r\n",
        "            else:\r\n",
        "                res.append(1/(1+np.exp(-z)))\r\n",
        "        return np.asarray(res)\r\n",
        "\r\n",
        "    def tanh(self,activations):\r\n",
        "        res = []\r\n",
        "        for z in activations:\r\n",
        "            if z>20:\r\n",
        "                res.append(1.0)\r\n",
        "            elif z<-20:\r\n",
        "                res.append(-1.0)\r\n",
        "            else:\r\n",
        "                res.append((np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z)))\r\n",
        "        return np.asarray(res)\r\n",
        "\r\n",
        "    def relu(self,activations):\r\n",
        "        res = []\r\n",
        "        for i in activations:\r\n",
        "            if i<= 0:\r\n",
        "                res.append(0)\r\n",
        "            else:\r\n",
        "                res.append(i)\r\n",
        "        return np.asarray(res)\r\n",
        "\r\n",
        "    def softmax(self,activations):\r\n",
        "        tot = 0\r\n",
        "        for z in activations:\r\n",
        "            tot += np.exp(z)\r\n",
        "        return np.asarray([np.exp(z)/tot for z in activations])\r\n",
        "\r\n",
        "    def onehot(self,y):\r\n",
        "      y_onehot = np.zeros(self.output_dim)\r\n",
        "      y_onehot[y] = 1\r\n",
        "      return y_onehot\r\n",
        "\r\n",
        "\r\n",
        "    def forward_propagation(self,x,y,weights,biases):\r\n",
        "        self.activations[0] = x\r\n",
        "        n = len(self.layers)\r\n",
        "        for i in range(n-2):\r\n",
        "            if self.activation_func == \"sigmoid\":\r\n",
        "                self.activations[i+1] = self.sigmoid(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n",
        "            elif self.activation_func == \"tanh\":\r\n",
        "                self.activations[i+1] = self.tanh(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n",
        "            elif self.activation_func == \"relu\":\r\n",
        "                self.activations[i+1] = self.relu(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n",
        "\r\n",
        "        self.activations[n-1] = self.softmax(np.matmul(weights[n-2].T,self.activations[n-2])+biases[n-2])  \r\n",
        "        if self.loss == \"cross_entropy\":      \r\n",
        "          return -(np.log2(self.activations[-1][y])) #Return cross entropy loss for single data point.\r\n",
        "        elif self.loss == \"squared_loss\":\r\n",
        "          y_onehot = self.onehot(y)\r\n",
        "          return np.sum(np.square(self.activations[-1] - y_onehot))\r\n",
        "\r\n",
        "    def grad_w(self,i):\r\n",
        "        return np.matmul(self.activations[i].reshape((-1,1)),self.activation_gradients[i].reshape((1,-1)))\r\n",
        "\r\n",
        "\r\n",
        "    def grad_b(self,i):\r\n",
        "        return self.activation_gradients[i]\r\n",
        "\r\n",
        "\r\n",
        "    def backward_propagation(self,x,y,weights,biases):\r\n",
        "        y_onehot = self.onehot(y)\r\n",
        "        n = len(self.layers)\r\n",
        "\r\n",
        "        if self.loss == \"cross_entropy\": \r\n",
        "          self.activation_gradients[-1] =  -1*(y_onehot - self.activations[-1])\r\n",
        "        elif self.loss == \"squared_loss\":\r\n",
        "          temp_vec = 2 * (self.activations[-1] - y_onehot) * (self.activations[-1])\r\n",
        "          for i in range(len(self.activations[-1])):\r\n",
        "            self.activation_gradients[-1][i] = np.dot(temp_vec,(self.onehot(i) - np.asarray([self.activations[-1][i]]*self.output_dim)))\r\n",
        "          \r\n",
        "        for i in range(n-2,-1,-1):\r\n",
        "            self.weights_gradients[i] += self.grad_w(i)\r\n",
        "            self.biases_gradients[i] += self.grad_b(i)\r\n",
        "            if i!=0:\r\n",
        "                value = np.matmul(weights[i],self.activation_gradients[i])\r\n",
        "                if self.activation_func == \"sigmoid\":\r\n",
        "                    self.activation_gradients[i-1] = value * self.activations[i] * (1-self.activations[i])\r\n",
        "                elif self.activation_func == \"tanh\":\r\n",
        "                    self.activation_gradients[i-1] = value * (1-np.square(self.activations[i]))\r\n",
        "                elif self.activation_func == \"relu\":\r\n",
        "                    res = []\r\n",
        "                    for k in self.activations[i]:\r\n",
        "                        if k>0: res.append(1.0)\r\n",
        "                        else: res.append(0.0)\r\n",
        "                    res = np.asarray(res)\r\n",
        "                    self.activation_gradients[i-1] = value * res\r\n",
        "\r\n",
        "    def gradient_descent(self,x_train,y_train):\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "            \r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        self.weights[j] -= self.learning_rate * self.weights_gradients[j]\r\n",
        "                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                index += 1 \r\n",
        "\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def sgd(self,x_train,y_train):\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        self.weights[j] -= self.learning_rate * self.weights_gradients[j]\r\n",
        "                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                index +=1\r\n",
        "\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "    \r\n",
        "    def momentum(self,x_train,y_train):\r\n",
        "        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        v_w = (self.decay_rate * prev_gradients_w[j] + self.learning_rate * self.weights_gradients[j])\r\n",
        "                        v_b = (self.decay_rate * prev_gradients_b[j] + self.learning_rate * self.biases_gradients[j])\r\n",
        "                        self.weights[j] -= v_w\r\n",
        "                        self.biases[j] -= v_b\r\n",
        "                        prev_gradients_w[j] = v_w\r\n",
        "                        prev_gradients_b[j] = v_b\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "                index +=1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def nesterov(self,x_train,y_train):\r\n",
        "        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            weights = [self.weights[j] -  self.decay_rate * prev_gradients_w[j] for j in range(len(self.weights))]\r\n",
        "            biases = [self.biases[j] -  self.decay_rate * prev_gradients_b[j] for j in range(len(self.biases))]\r\n",
        "\r\n",
        "            self.weights_gradients = [0*j for j in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*j for j in (self.biases_gradients)]\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,weights,biases)\r\n",
        "                self.backward_propagation(x,y,weights,biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        prev_gradients_w[j] = self.decay_rate * prev_gradients_w[j] + self.learning_rate*self.weights_gradients[j] \r\n",
        "                                           \r\n",
        "                        prev_gradients_b[j] = self.decay_rate * prev_gradients_b[j] + self.learning_rate*self.biases_gradients[j] \r\n",
        "                                        \r\n",
        "                        self.weights[j] -= prev_gradients_w[j]\r\n",
        "                        self.biases[j] -= prev_gradients_b[j]\r\n",
        "\r\n",
        "                    weights = [self.weights[j] -  self.decay_rate * prev_gradients_w[j] for j in range(len(self.weights))]\r\n",
        "                    biases = [self.biases[j] -  self.decay_rate * prev_gradients_b[j] for j in range(len(self.biases))]\r\n",
        "\r\n",
        "                    self.weights_gradients = [0*j for j in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*j for j in (self.biases_gradients)]\r\n",
        "                    \r\n",
        "                index += 1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def rmsprop(self,x_train,y_train):\r\n",
        "        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "        eps = 1e-2\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        v_w = (self.beta * prev_gradients_w[j] + (1-self.beta) * np.square(self.weights_gradients[j]))\r\n",
        "                        v_b = (self.beta * prev_gradients_b[j] + (1-self.beta) * np.square(self.biases_gradients[j]))\r\n",
        "                        self.weights[j] -= self.learning_rate * (self.weights_gradients[j] /(np.sqrt(v_w + eps)))\r\n",
        "                        self.biases[j] -= self.learning_rate * (self.biases_gradients[j] /(np.sqrt(v_b + eps)))\r\n",
        "                        prev_gradients_w[j] = v_w\r\n",
        "                        prev_gradients_b[j] = v_b\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "                index +=1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def adam(self,x_train,y_train):\r\n",
        "        m_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        m_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "        v_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        v_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        iter = 1\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "            eps = 1e-2\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j] + (1-self.beta1) * self.weights_gradients[j])\r\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j] + (1-self.beta1) * self.biases_gradients[j])\r\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j] + (1-self.beta2) * np.square(self.weights_gradients[j]))\r\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j] + (1-self.beta2) * np.square(self.biases_gradients[j]))\r\n",
        "\r\n",
        "                        m_hat_w = (m_w)/(1-(self.beta1)**iter) \r\n",
        "                        m_hat_b = (m_b)/(1-(self.beta1)**iter) \r\n",
        "\r\n",
        "                        v_hat_w = (v_w)/(1-(self.beta2)**iter) \r\n",
        "                        v_hat_b = (v_b)/(1-(self.beta2)**iter)\r\n",
        "\r\n",
        "                        self.weights[j] -= self.learning_rate * (m_hat_w/(np.sqrt(v_hat_w + eps)))\r\n",
        "                        self.biases[j] -= self.learning_rate * (m_hat_b/(np.sqrt(v_hat_b + eps)))\r\n",
        "\r\n",
        "                        m_prev_gradients_w[j] = m_w\r\n",
        "                        m_prev_gradients_b[j] = m_b\r\n",
        "                        v_prev_gradients_w[j] = v_w\r\n",
        "                        v_prev_gradients_b[j] = v_b\r\n",
        "\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                    iter += 1\r\n",
        "\r\n",
        "                index +=1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "    def nadam(self,x_train,y_train):\r\n",
        "        m_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        m_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "        v_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        v_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        iter = 1\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "            eps = 1e-2\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "\r\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j] + (1-self.beta1) * self.weights_gradients[j])\r\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j] + (1-self.beta1) * self.biases_gradients[j])\r\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j] + (1-self.beta2) * np.square(self.weights_gradients[j]))\r\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j] + (1-self.beta2) * np.square(self.biases_gradients[j]))\r\n",
        "\r\n",
        "                        m_hat_w = (m_w)/(1-(self.beta1)**iter) \r\n",
        "                        m_hat_b = (m_b)/(1-(self.beta1)**iter) \r\n",
        "\r\n",
        "                        v_hat_w = (v_w)/(1-(self.beta2)**iter) \r\n",
        "                        v_hat_b = (v_b)/(1-(self.beta2)**iter)\r\n",
        "\r\n",
        "                        m_dash_w = self.beta1 * m_hat_w + (1-self.beta1) * self.weights_gradients[j]\r\n",
        "                        m_dash_b = self.beta1 * m_hat_b + (1-self.beta1) * self.biases_gradients[j]\r\n",
        "\r\n",
        "                        self.weights[j] -= self.learning_rate * (m_dash_w/(np.sqrt(v_hat_w + eps)))\r\n",
        "                        self.biases[j] -= self.learning_rate * (m_dash_b/(np.sqrt(v_hat_b + eps)))\r\n",
        "\r\n",
        "                        m_prev_gradients_w[j] = m_w\r\n",
        "                        m_prev_gradients_b[j] = m_b\r\n",
        "                        v_prev_gradients_w[j] = v_w\r\n",
        "                        v_prev_gradients_b[j] = v_b\r\n",
        "\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                    iter += 1\r\n",
        "\r\n",
        "                index +=1\r\n",
        "\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def calculate_accuracy(self,X,Y):\r\n",
        "        count = 0\r\n",
        "        for i in range(len(X)):\r\n",
        "            if self.predict(X[i]) == Y[i]:\r\n",
        "                count+=1\r\n",
        "        return count/len(X)\r\n",
        "\r\n",
        "\r\n",
        "    def predict(self,x):\r\n",
        "        x = x.ravel()\r\n",
        "        self.activations[0] = x\r\n",
        "        n = len(self.layers)\r\n",
        "        for i in range(n-2):\r\n",
        "            if self.activation_func == \"sigmoid\":\r\n",
        "                self.activations[i+1] = self.sigmoid(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n",
        "            elif self.activation_func == \"tanh\":\r\n",
        "                self.activations[i+1] = self.tanh(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n",
        "            elif self.activation_func == \"relu\":\r\n",
        "                self.activations[i+1] = self.relu(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n",
        "\r\n",
        "        self.activations[n-1] = self.softmax(np.matmul(self.weights[n-2].T,self.activations[n-2])+self.biases[n-2])\r\n",
        "\r\n",
        "        return np.argmax(self.activations[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kWVn-x4dHB-",
        "outputId": "91d8e399-511a-4728-b708-20366237712f"
      },
      "source": [
        "nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=2e-3,batch_size= 64,epochs=4,\r\n",
        "                    activation_func=\"tanh\",optimizer=\"nadam\",weight_init=\"xavier\",decay_rate=0.1,loss=\"cross_entropy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch--- 1   loss =  0.4835084137160567   accuracy =  0.956    validation loss=  0.23237722583959722   validation accuaracy=  0.954\n",
            "Epoch--- 2   loss =  0.1853922874067513   accuracy =  0.972    validation loss=  0.17443336719876837   validation accuaracy=  0.964\n",
            "Epoch--- 3   loss =  0.13105919448574613   accuracy =  0.977    validation loss=  0.15716057462496147   validation accuaracy=  0.965\n",
            "Epoch--- 4   loss =  0.10077285078510072   accuracy =  0.981    validation loss=  0.153917485180905   validation accuaracy=  0.969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjZGDw4UcDYz",
        "outputId": "9f158cae-a66f-4d80-98f0-7b0512084ede"
      },
      "source": [
        "print(\"The test accuracy is:\",nn.calculate_accuracy(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test accuracy is: 0.9698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCcwra12YuOD",
        "outputId": "ca68c638-a5d4-4b16-a4a5-8ab6955dd547"
      },
      "source": [
        "nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=5e-3,batch_size= 32,epochs=7,\r\n",
        "                    activation_func=\"relu\",optimizer=\"adam\",weight_init=\"xavier\",decay_rate=0.1,loss=\"cross_entropy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch--- 1   loss =  0.4234705637182151   accuracy =  0.963    validation loss=  0.2169104144961324   validation accuaracy=  0.958\n",
            "Epoch--- 2   loss =  0.1982155991277626   accuracy =  0.969    validation loss=  0.19622833178210855   validation accuaracy=  0.961\n",
            "Epoch--- 3   loss =  0.16354087164715472   accuracy =  0.972    validation loss=  0.21294414896804723   validation accuaracy=  0.962\n",
            "Epoch--- 4   loss =  0.14172612311615249   accuracy =  0.975    validation loss=  0.18689424417900727   validation accuaracy=  0.967\n",
            "Epoch--- 5   loss =  0.12834052997460701   accuracy =  0.975    validation loss=  0.22939953495103318   validation accuaracy=  0.965\n",
            "Epoch--- 6   loss =  0.11713228642879166   accuracy =  0.977    validation loss=  0.2670011708656827   validation accuaracy=  0.963\n",
            "Epoch--- 7   loss =  0.11035668156973186   accuracy =  0.969    validation loss=  0.3279154669646567   validation accuaracy=  0.957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgI1ESimbzlB",
        "outputId": "f1002c15-6ef2-4d88-f00a-d2401210f022"
      },
      "source": [
        "print(\"The test accuracy is:\",nn.calculate_accuracy(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test accuracy is: 0.9532\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-V8r8SFXrPR",
        "outputId": "4bac032d-d9ca-4f29-fbf9-4384a8d588eb"
      },
      "source": [
        "nn = Neural_network(x_train,y_train,784,16,3,10,learning_rate=6e-3,batch_size= 32,epochs=5,\r\n",
        "                    activation_func=\"sigmoid\",optimizer=\"rmsprop\",weight_init=\"random\",decay_rate=0.1,loss=\"cross_entropy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch--- 1   loss =  1.0700620715642715   accuracy =  0.903    validation loss=  0.53128106738805   validation accuaracy=  0.897\n",
            "Epoch--- 2   loss =  0.4558863616636201   accuracy =  0.926    validation loss=  0.4167704747563029   validation accuaracy=  0.92\n",
            "Epoch--- 3   loss =  0.3741814532825794   accuracy =  0.932    validation loss=  0.39215008379322064   validation accuaracy=  0.923\n",
            "Epoch--- 4   loss =  0.3306393792559905   accuracy =  0.936    validation loss=  0.3767928306808907   validation accuaracy=  0.925\n",
            "Epoch--- 5   loss =  0.30162908091960217   accuracy =  0.938    validation loss=  0.37701833994254047   validation accuaracy=  0.927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcqvPKUwYY4A",
        "outputId": "72cf1de9-1227-4b9d-8551-43183d1d3850"
      },
      "source": [
        "print(\"The test accuracy is:\",nn.calculate_accuracy(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The test accuracy is: 0.9279\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}