{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Question 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAudwcC1-tgy",
        "outputId": "93b46081-50db-4047-e0e8-9d9ab52ad46e"
      },
      "source": [
        "from keras.datasets import fashion_mnist\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()\r\n",
        "x_train = x_train/255\r\n",
        "x_test = x_test/255"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QF7-2Al-t51"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "class Neural_network:\r\n",
        "    np.random.seed(10)\r\n",
        "    def __init__(self,x_train,y_train,input_dim,hidden_layers_size,hidden_layers,output_dim,batch_size=30,epochs=10,activation_func=\"relu\"\r\n",
        "           ,learning_rate=6e-3 ,decay_rate=0.9,beta=0.9,beta1=0.9,beta2=0.99,optimizer=\"adam\",weight_init=\"xavier\"):\r\n",
        "\r\n",
        "        self.x_train,self.x_cv,self.y_train,self.y_cv = train_test_split(x_train, y_train, test_size=0.10, random_state=100,stratify=y_train)\r\n",
        "\r\n",
        "        np.random.seed(10)\r\n",
        "        self.input_dim = input_dim\r\n",
        "        self.hidden_layers = hidden_layers\r\n",
        "        self.hidden_layers_size = hidden_layers_size\r\n",
        "        self.output_dim = output_dim\r\n",
        "\r\n",
        "        self.batch = batch_size\r\n",
        "        self.epochs = epochs\r\n",
        "        self.activation_func = activation_func\r\n",
        "        self.learning_rate = learning_rate\r\n",
        "        self.decay_rate = decay_rate\r\n",
        "        self.optimizer = optimizer\r\n",
        "        self.weight_init = weight_init\r\n",
        "        self.beta = beta\r\n",
        "        self.beta1 = beta1\r\n",
        "        self.beta2 = beta2\r\n",
        "\r\n",
        "        self.layers = [self.input_dim] + self.hidden_layers*[self.hidden_layers_size] + [self.output_dim]\r\n",
        "\r\n",
        "        layers = self.layers.copy()\r\n",
        "        self.weights = []\r\n",
        "        self.biases = []\r\n",
        "        self.activations = []\r\n",
        "        self.activation_gradients = []\r\n",
        "        self.weights_gradients = []\r\n",
        "        self.biases_gradients = []\r\n",
        "\r\n",
        "        for i in range(len(layers)-1):\r\n",
        "            if self.weight_init == 'xavier':\r\n",
        "                std = np.sqrt(2/(layers[i]*layers[i+1]))\r\n",
        "                self.weights.append(np.random.normal(0,std,(layers[i],layers[i+1])))\r\n",
        "                self.biases.append(np.random.normal(0,std,(layers[i+1])))\r\n",
        "            else:\r\n",
        "                self.weights.append(np.random.normal(0,0.5,(layers[i],layers[i+1])))\r\n",
        "                self.biases.append(np.random.normal(0,0.5,(layers[i+1])))\r\n",
        "            self.activations.append(np.zeros(layers[i]))\r\n",
        "            self.activation_gradients.append(np.zeros(layers[i+1]))\r\n",
        "            self.weights_gradients.append(np.zeros((layers[i],layers[i+1])))\r\n",
        "            self.biases_gradients.append(np.zeros(layers[i+1]))\r\n",
        "\r\n",
        "        self.activations.append(np.zeros(layers[-1]))\r\n",
        "        \r\n",
        "        if optimizer == 'adam':\r\n",
        "            self.adam(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'sgd':\r\n",
        "            self.sgd(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'momentum':\r\n",
        "            self.momentum(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'nesterov':\r\n",
        "            self.nesterov(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'nadam':\r\n",
        "            self.nadam(self.x_train,self.y_train)\r\n",
        "        elif optimizer == 'rmsprop':\r\n",
        "            self.rmsprop(self.x_train,self.y_train)\r\n",
        "\r\n",
        "\r\n",
        "    def sigmoid(self,activations):\r\n",
        "        res = []\r\n",
        "        for z in activations:\r\n",
        "            if z>40:\r\n",
        "                res.append(1.0)\r\n",
        "            elif z<-40:\r\n",
        "                res.append(0.0)\r\n",
        "            else:\r\n",
        "                res.append(1/(1+np.exp(-z)))\r\n",
        "        return np.asarray(res)\r\n",
        "\r\n",
        "    def tanh(self,activations):\r\n",
        "        res = []\r\n",
        "        for z in activations:\r\n",
        "            if z>20:\r\n",
        "                res.append(1.0)\r\n",
        "            elif z<-20:\r\n",
        "                res.append(-1.0)\r\n",
        "            else:\r\n",
        "                res.append((np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z)))\r\n",
        "        return np.asarray(res)\r\n",
        "\r\n",
        "    def relu(self,activations):\r\n",
        "        res = []\r\n",
        "        for i in activations:\r\n",
        "            if i<= 0:\r\n",
        "                res.append(0)\r\n",
        "            else:\r\n",
        "                res.append(i)\r\n",
        "        return np.asarray(res)\r\n",
        "\r\n",
        "    def softmax(self,activations):\r\n",
        "        tot = 0\r\n",
        "        for z in activations:\r\n",
        "            tot += np.exp(z)\r\n",
        "        return np.asarray([np.exp(z)/tot for z in activations])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def forward_propagation(self,x,y,weights,biases):\r\n",
        "        self.activations[0] = x\r\n",
        "        n = len(self.layers)\r\n",
        "        for i in range(n-2):\r\n",
        "            if self.activation_func == \"sigmoid\":\r\n",
        "                self.activations[i+1] = self.sigmoid(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n",
        "            elif self.activation_func == \"tanh\":\r\n",
        "                self.activations[i+1] = self.tanh(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n",
        "            elif self.activation_func == \"relu\":\r\n",
        "                self.activations[i+1] = self.relu(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n",
        "\r\n",
        "        self.activations[n-1] = self.softmax(np.matmul(weights[n-2].T,self.activations[n-2])+biases[n-2])        \r\n",
        "        return -(np.log2(self.activations[-1][y])) #Return cross entropy loss for single data point.\r\n",
        "\r\n",
        "    def grad_w(self,i):\r\n",
        "        return np.matmul(self.activations[i].reshape((-1,1)),self.activation_gradients[i].reshape((1,-1)))\r\n",
        "\r\n",
        "\r\n",
        "    def grad_b(self,i):\r\n",
        "        return self.activation_gradients[i]\r\n",
        "\r\n",
        "\r\n",
        "    def backward_propagation(self,x,y,weights,biases):\r\n",
        "        y_onehot = np.zeros(self.output_dim)\r\n",
        "        y_onehot[y] = 1\r\n",
        "        n = len(self.layers)\r\n",
        "\r\n",
        "        self.activation_gradients[-1] =  -1*(y_onehot - self.activations[-1])\r\n",
        "        for i in range(n-2,-1,-1):\r\n",
        "            self.weights_gradients[i] += self.grad_w(i)\r\n",
        "            self.biases_gradients[i] += self.grad_b(i)\r\n",
        "            if i!=0:\r\n",
        "                value = np.matmul(weights[i],self.activation_gradients[i])\r\n",
        "                if self.activation_func == \"sigmoid\":\r\n",
        "                    self.activation_gradients[i-1] = value * self.activations[i] * (1-self.activations[i])\r\n",
        "                elif self.activation_func == \"tanh\":\r\n",
        "                    self.activation_gradients[i-1] = value * (1-np.square(self.activations[i]))\r\n",
        "                elif self.activation_func == \"relu\":\r\n",
        "                    res = []\r\n",
        "                    for k in self.activations[i]:\r\n",
        "                        if k>0: res.append(1.0)\r\n",
        "                        else: res.append(0.0)\r\n",
        "                    res = np.asarray(res)\r\n",
        "                    self.activation_gradients[i-1] = value * res\r\n",
        "\r\n",
        "    def gradient_descent(self,x_train,y_train):\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "            \r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        self.weights[j] -= self.learning_rate * self.weights_gradients[j]\r\n",
        "                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                index += 1 \r\n",
        "\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',round(self.calculate_accuracy(x_train,y_train),3))\r\n",
        "\r\n",
        "\r\n",
        "    def sgd(self,x_train,y_train):\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        self.weights[j] -= self.learning_rate * self.weights_gradients[j]\r\n",
        "                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                index+=1\r\n",
        "\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "    \r\n",
        "    def momentum(self,x_train,y_train):\r\n",
        "        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        v_w = (self.decay_rate * prev_gradients_w[j] + self.learning_rate * self.weights_gradients[j])\r\n",
        "                        v_b = (self.decay_rate * prev_gradients_b[j] + self.learning_rate * self.biases_gradients[j])\r\n",
        "                        self.weights[j] -= v_w\r\n",
        "                        self.biases[j] -= v_b\r\n",
        "                        prev_gradients_w[j] = v_w\r\n",
        "                        prev_gradients_b[j] = v_b\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "                index +=1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def nesterov(self,x_train,y_train):\r\n",
        "        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            weights = [self.weights[j] -  self.decay_rate * prev_gradients_w[j] for j in range(len(self.weights))]\r\n",
        "            biases = [self.biases[j] -  self.decay_rate * prev_gradients_b[j] for j in range(len(self.biases))]\r\n",
        "\r\n",
        "            self.weights_gradients = [0*j for j in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*j for j in (self.biases_gradients)]\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,weights,biases)\r\n",
        "                self.backward_propagation(x,y,weights,biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        prev_gradients_w[j] = self.decay_rate * prev_gradients_w[j] + self.learning_rate*self.weights_gradients[j] \r\n",
        "                                           \r\n",
        "                        prev_gradients_b[j] = self.decay_rate * prev_gradients_b[j] + self.learning_rate*self.biases_gradients[j] \r\n",
        "                                        \r\n",
        "                        self.weights[j] -= prev_gradients_w[j]\r\n",
        "                        self.biases[j] -= prev_gradients_b[j]\r\n",
        "\r\n",
        "                    weights = [self.weights[j] -  self.decay_rate * prev_gradients_w[j] for j in range(len(self.weights))]\r\n",
        "                    biases = [self.biases[j] -  self.decay_rate * prev_gradients_b[j] for j in range(len(self.biases))]\r\n",
        "\r\n",
        "                    self.weights_gradients = [0*j for j in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*j for j in (self.biases_gradients)]\r\n",
        "                    \r\n",
        "                index += 1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def rmsprop(self,x_train,y_train):\r\n",
        "        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "        eps = 1e-2\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        v_w = (self.beta * prev_gradients_w[j] + (1-self.beta) * np.square(self.weights_gradients[j]))\r\n",
        "                        v_b = (self.beta * prev_gradients_b[j] + (1-self.beta) * np.square(self.biases_gradients[j]))\r\n",
        "                        self.weights[j] -= self.learning_rate * (self.weights_gradients[j] /(np.sqrt(v_w + eps)))\r\n",
        "                        self.biases[j] -= self.learning_rate * (self.biases_gradients[j] /(np.sqrt(v_b + eps)))\r\n",
        "                        prev_gradients_w[j] = v_w\r\n",
        "                        prev_gradients_b[j] = v_b\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "                index +=1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def adam(self,x_train,y_train):\r\n",
        "        m_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        m_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "        v_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        v_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        iter = 1\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "            eps = 1e-2\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j] + (1-self.beta1) * self.weights_gradients[j])\r\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j] + (1-self.beta1) * self.biases_gradients[j])\r\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j] + (1-self.beta2) * np.square(self.weights_gradients[j]))\r\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j] + (1-self.beta2) * np.square(self.biases_gradients[j]))\r\n",
        "\r\n",
        "                        m_hat_w = (m_w)/(1-(self.beta1)**iter) \r\n",
        "                        m_hat_b = (m_b)/(1-(self.beta1)**iter) \r\n",
        "\r\n",
        "                        v_hat_w = (v_w)/(1-(self.beta2)**iter) \r\n",
        "                        v_hat_b = (v_b)/(1-(self.beta2)**iter)\r\n",
        "\r\n",
        "                        self.weights[j] -= self.learning_rate * (m_hat_w/(np.sqrt(v_hat_w + eps)))\r\n",
        "                        self.biases[j] -= self.learning_rate * (m_hat_b/(np.sqrt(v_hat_b + eps)))\r\n",
        "\r\n",
        "                        m_prev_gradients_w[j] = m_w\r\n",
        "                        m_prev_gradients_b[j] = m_b\r\n",
        "                        v_prev_gradients_w[j] = v_w\r\n",
        "                        v_prev_gradients_b[j] = v_b\r\n",
        "\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                    iter += 1\r\n",
        "\r\n",
        "                index +=1\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "    def nadam(self,x_train,y_train):\r\n",
        "        m_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        m_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "        v_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n",
        "        v_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "        iter = 1\r\n",
        "\r\n",
        "        for i in range(self.epochs):\r\n",
        "            print('Epoch---',i+1,end=\" \")\r\n",
        "            loss = 0\r\n",
        "            val_loss=0\r\n",
        "            eps = 1e-2\r\n",
        "            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "\r\n",
        "            index = 1\r\n",
        "            for x,y in zip(x_train,y_train):\r\n",
        "                x = x.ravel()\r\n",
        "                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "                self.backward_propagation(x,y,self.weights,self.biases)\r\n",
        "                if index % self.batch == 0 or index == x_train.shape[0]:\r\n",
        "                    for j in range(len(self.weights)):\r\n",
        "\r\n",
        "                        m_w = (self.beta1 * m_prev_gradients_w[j] + (1-self.beta1) * self.weights_gradients[j])\r\n",
        "                        m_b = (self.beta1 * m_prev_gradients_b[j] + (1-self.beta1) * self.biases_gradients[j])\r\n",
        "                        v_w = (self.beta2 * v_prev_gradients_w[j] + (1-self.beta2) * np.square(self.weights_gradients[j]))\r\n",
        "                        v_b = (self.beta2 * v_prev_gradients_b[j] + (1-self.beta2) * np.square(self.biases_gradients[j]))\r\n",
        "\r\n",
        "                        m_hat_w = (m_w)/(1-(self.beta1)**iter) \r\n",
        "                        m_hat_b = (m_b)/(1-(self.beta1)**iter) \r\n",
        "\r\n",
        "                        v_hat_w = (v_w)/(1-(self.beta2)**iter) \r\n",
        "                        v_hat_b = (v_b)/(1-(self.beta2)**iter)\r\n",
        "\r\n",
        "                        m_dash_w = self.beta1 * m_hat_w + (1-self.beta1) * self.weights_gradients[j]\r\n",
        "                        m_dash_b = self.beta1 * m_hat_b + (1-self.beta1) * self.biases_gradients[j]\r\n",
        "\r\n",
        "                        self.weights[j] -= self.learning_rate * (m_dash_w/(np.sqrt(v_hat_w + eps)))\r\n",
        "                        self.biases[j] -= self.learning_rate * (m_dash_b/(np.sqrt(v_hat_b + eps)))\r\n",
        "\r\n",
        "                        m_prev_gradients_w[j] = m_w\r\n",
        "                        m_prev_gradients_b[j] = m_b\r\n",
        "                        v_prev_gradients_w[j] = v_w\r\n",
        "                        v_prev_gradients_b[j] = v_b\r\n",
        "\r\n",
        "                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n",
        "                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n",
        "                    iter += 1\r\n",
        "\r\n",
        "                index +=1\r\n",
        "\r\n",
        "            for x,y in zip(self.x_cv,self.y_cv):\r\n",
        "               x=x.ravel()\r\n",
        "               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n",
        "\r\n",
        "            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n",
        "            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n",
        "            wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n",
        "            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n",
        "\r\n",
        "\r\n",
        "    def calculate_accuracy(self,X,Y):\r\n",
        "        count = 0\r\n",
        "        for i in range(len(X)):\r\n",
        "            if self.predict(X[i]) == Y[i]:\r\n",
        "                count+=1\r\n",
        "        return count/len(X)\r\n",
        "\r\n",
        "\r\n",
        "    def predict(self,x):\r\n",
        "        x = x.ravel()\r\n",
        "        self.activations[0] = x\r\n",
        "        n = len(self.layers)\r\n",
        "        for i in range(n-2):\r\n",
        "            if self.activation_func == \"sigmoid\":\r\n",
        "                self.activations[i+1] = self.sigmoid(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n",
        "            elif self.activation_func == \"tanh\":\r\n",
        "                self.activations[i+1] = self.tanh(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n",
        "            elif self.activation_func == \"relu\":\r\n",
        "                self.activations[i+1] = self.relu(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n",
        "\r\n",
        "        self.activations[n-1] = self.softmax(np.matmul(self.weights[n-2].T,self.activations[n-2])+self.biases[n-2])\r\n",
        "\r\n",
        "        return np.argmax(self.activations[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwOYN0TJ-t8W"
      },
      "source": [
        "sweep_config={\r\n",
        "    'method': 'random',\r\n",
        "    'metric': {\r\n",
        "        'name': 'accuracy',\r\n",
        "        'goal': 'maximize'\r\n",
        "    },\r\n",
        "    'parameters':{\r\n",
        "        'epochs':{\r\n",
        "            'values':[3,5,7]\r\n",
        "        },\r\n",
        "        'batch_size':{\r\n",
        "            'values':[32,64,128]\r\n",
        "        },\r\n",
        "        'hidden_layers':{\r\n",
        "            'values':[1,2,3]\r\n",
        "        },\r\n",
        "        'hidden_layers_size':{\r\n",
        "            'values':[16,32,64]\r\n",
        "        },\r\n",
        "        'learning_rate':{\r\n",
        "            'values':[5e-3,2e-3,6e-3,5e-4]\r\n",
        "        },\r\n",
        "        'weight_decay':{\r\n",
        "            'values':[0.1,0.0,0.8,0.9]\r\n",
        "        },\r\n",
        "        'optimizer':{\r\n",
        "            'values':['sgd','momentum','nesterov','adam','rmsprop','nadam']\r\n",
        "        },\r\n",
        "        'activation':{\r\n",
        "            'values':['sigmoid','tanh','relu']\r\n",
        "        },\r\n",
        "        'weight_init':{\r\n",
        "            'values':['random','xavier']\r\n",
        "        }\r\n",
        "    }\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KevLCZPV-uDH"
      },
      "source": [
        "!pip install --upgrade wandb\r\n",
        "import wandb\r\n",
        "!wandb login 3c967c63b099a3b2acd600aa30008e7de1ea6498"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVzBtISB-uFf",
        "outputId": "98e68f64-f313-4f65-d955-e9973f29259c"
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config,project=\"fashion_mnist\", entity=\"adi00510\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: d4znbkb2\n",
            "Sweep URL: https://wandb.ai/adi00510/fashion_mnist/sweeps/d4znbkb2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNNOaxUE-uHy"
      },
      "source": [
        "def train():\r\n",
        "    config_defaults={\r\n",
        "      'epochs':5,\r\n",
        "      'batch_size':16,\r\n",
        "      'learning_rate':1e-3,\r\n",
        "      'activation':'relu',\r\n",
        "      'optimizer':'adam',\r\n",
        "      'hidden_layers_size':32,\r\n",
        "      'hidden_layers':3,\r\n",
        "      'weight_init':'xavier' }\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    wandb.init(config=config_defaults)\r\n",
        "    config=wandb.config\r\n",
        "\r\n",
        "    Neural_network(x_train,y_train,len(x_train[0].ravel()),config.hidden_layers_size,config.hidden_layers,max(y_train)+1,\r\n",
        "                       config.batch_size,config.epochs,config.activation,\r\n",
        "                 config.learning_rate,config.weight_decay,0.9,0.9,0.99\r\n",
        "                        ,config.optimizer,config.weight_init)\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oevVD_pF-uJ7"
      },
      "source": [
        "wandb.agent(sweep_id,train)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}