{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Question_8.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMrzGvUNIGuTq56s84df05J"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"pAudwcC1-tgy","executionInfo":{"status":"ok","timestamp":1615615389965,"user_tz":-330,"elapsed":1153,"user":{"displayName":"Varun Sai Dakavaram cs20m069","photoUrl":"","userId":"01521556991466858861"}}},"source":["from keras.datasets import fashion_mnist\r\n","from matplotlib import pyplot as plt\r\n","\r\n","\r\n","(x_train,y_train),(x_test,y_test)=fashion_mnist.load_data()\r\n","x_train = x_train/255\r\n","x_test = x_test/255"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"2QF7-2Al-t51","executionInfo":{"status":"ok","timestamp":1615617261334,"user_tz":-330,"elapsed":4937,"user":{"displayName":"Varun Sai Dakavaram cs20m069","photoUrl":"","userId":"01521556991466858861"}}},"source":["import numpy as np\r\n","import pandas as pd\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","class Neural_network:\r\n","    np.random.seed(10)\r\n","    def __init__(self,x_train,y_train,input_dim,hidden_layers_size,hidden_layers,output_dim,batch_size=30,epochs=10,activation_func=\"relu\"\r\n","           ,learning_rate=6e-3 ,decay_rate=0.9,beta=0.9,beta1=0.9,beta2=0.99,optimizer=\"adam\",weight_init=\"xavier\",loss=\"cross_entropy\"):\r\n","\r\n","        self.x_train,self.x_cv,self.y_train,self.y_cv = train_test_split(x_train, y_train, test_size=0.10, random_state=100,stratify=y_train)\r\n","\r\n","        np.random.seed(10)\r\n","        self.input_dim = input_dim\r\n","        self.hidden_layers = hidden_layers\r\n","        self.hidden_layers_size = hidden_layers_size\r\n","        self.output_dim = output_dim\r\n","\r\n","        self.batch = batch_size\r\n","        self.epochs = epochs\r\n","        self.activation_func = activation_func\r\n","        self.learning_rate = learning_rate\r\n","        self.decay_rate = decay_rate\r\n","        self.optimizer = optimizer\r\n","        self.weight_init = weight_init\r\n","        self.beta = beta\r\n","        self.beta1 = beta1\r\n","        self.beta2 = beta2\r\n","        self.loss = loss\r\n","\r\n","        self.layers = [self.input_dim] + self.hidden_layers*[self.hidden_layers_size] + [self.output_dim]\r\n","\r\n","        layers = self.layers.copy()\r\n","        self.weights = []\r\n","        self.biases = []\r\n","        self.activations = []\r\n","        self.activation_gradients = []\r\n","        self.weights_gradients = []\r\n","        self.biases_gradients = []\r\n","\r\n","        for i in range(len(layers)-1):\r\n","            if self.weight_init == 'xavier':\r\n","                std = np.sqrt(2/(layers[i]*layers[i+1]))\r\n","                self.weights.append(np.random.normal(0,std,(layers[i],layers[i+1])))\r\n","                self.biases.append(np.random.normal(0,std,(layers[i+1])))\r\n","            else:\r\n","                self.weights.append(np.random.normal(0,0.5,(layers[i],layers[i+1])))\r\n","                self.biases.append(np.random.normal(0,0.5,(layers[i+1])))\r\n","            self.activations.append(np.zeros(layers[i]))\r\n","            self.activation_gradients.append(np.zeros(layers[i+1]))\r\n","            self.weights_gradients.append(np.zeros((layers[i],layers[i+1])))\r\n","            self.biases_gradients.append(np.zeros(layers[i+1]))\r\n","\r\n","        self.activations.append(np.zeros(layers[-1]))\r\n","        \r\n","        if optimizer == 'adam':\r\n","            self.adam(self.x_train,self.y_train)\r\n","        elif optimizer == 'sgd':\r\n","            self.sgd(self.x_train,self.y_train)\r\n","        elif optimizer == 'momentum':\r\n","            self.momentum(self.x_train,self.y_train)\r\n","        elif optimizer == 'nesterov':\r\n","            self.nesterov(self.x_train,self.y_train)\r\n","        elif optimizer == 'nadam':\r\n","            self.nadam(self.x_train,self.y_train)\r\n","        elif optimizer == 'rmsprop':\r\n","            self.rmsprop(self.x_train,self.y_train)\r\n","\r\n","\r\n","    def sigmoid(self,activations):\r\n","        res = []\r\n","        for z in activations:\r\n","            if z>40:\r\n","                res.append(1.0)\r\n","            elif z<-40:\r\n","                res.append(0.0)\r\n","            else:\r\n","                res.append(1/(1+np.exp(-z)))\r\n","        return np.asarray(res)\r\n","\r\n","    def tanh(self,activations):\r\n","        res = []\r\n","        for z in activations:\r\n","            if z>20:\r\n","                res.append(1.0)\r\n","            elif z<-20:\r\n","                res.append(-1.0)\r\n","            else:\r\n","                res.append((np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z)))\r\n","        return np.asarray(res)\r\n","\r\n","    def relu(self,activations):\r\n","        res = []\r\n","        for i in activations:\r\n","            if i<= 0:\r\n","                res.append(0)\r\n","            else:\r\n","                res.append(i)\r\n","        return np.asarray(res)\r\n","\r\n","    def softmax(self,activations):\r\n","        tot = 0\r\n","        for z in activations:\r\n","            tot += np.exp(z)\r\n","        return np.asarray([np.exp(z)/tot for z in activations])\r\n","\r\n","    def onehot(self,y):\r\n","      y_onehot = np.zeros(self.output_dim)\r\n","      y_onehot[y] = 1\r\n","      return y_onehot\r\n","\r\n","\r\n","    def forward_propagation(self,x,y,weights,biases):\r\n","        self.activations[0] = x\r\n","        n = len(self.layers)\r\n","        for i in range(n-2):\r\n","            if self.activation_func == \"sigmoid\":\r\n","                self.activations[i+1] = self.sigmoid(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n","            elif self.activation_func == \"tanh\":\r\n","                self.activations[i+1] = self.tanh(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n","            elif self.activation_func == \"relu\":\r\n","                self.activations[i+1] = self.relu(np.matmul(weights[i].T,self.activations[i])+biases[i])\r\n","\r\n","        self.activations[n-1] = self.softmax(np.matmul(weights[n-2].T,self.activations[n-2])+biases[n-2])  \r\n","        if self.loss == \"cross_entropy\":      \r\n","          return -(np.log2(self.activations[-1][y])) #Return cross entropy loss for single data point.\r\n","        elif self.loss == \"squared_loss\":\r\n","          y_onehot = self.onehot(y)\r\n","          return np.sum(np.square(self.activations[-1] - y_onehot))\r\n","\r\n","    def grad_w(self,i):\r\n","        return np.matmul(self.activations[i].reshape((-1,1)),self.activation_gradients[i].reshape((1,-1)))\r\n","\r\n","\r\n","    def grad_b(self,i):\r\n","        return self.activation_gradients[i]\r\n","\r\n","\r\n","    def backward_propagation(self,x,y,weights,biases):\r\n","        y_onehot = self.onehot(y)\r\n","        n = len(self.layers)\r\n","\r\n","        if self.loss == \"cross_entropy\": \r\n","          self.activation_gradients[-1] =  -1*(y_onehot - self.activations[-1])\r\n","        elif self.loss == \"squared_loss\":\r\n","          temp_vec = 2 * (self.activations[-1] - y_onehot) * (self.activations[-1])\r\n","          for i in range(len(self.activations[-1])):\r\n","            self.activation_gradients[-1][i] = np.dot(temp_vec,(self.onehot(i) - np.asarray([self.activations[-1][i]]*self.output_dim)))\r\n","          \r\n","        for i in range(n-2,-1,-1):\r\n","            self.weights_gradients[i] += self.grad_w(i)\r\n","            self.biases_gradients[i] += self.grad_b(i)\r\n","            if i!=0:\r\n","                value = np.matmul(weights[i],self.activation_gradients[i])\r\n","                if self.activation_func == \"sigmoid\":\r\n","                    self.activation_gradients[i-1] = value * self.activations[i] * (1-self.activations[i])\r\n","                elif self.activation_func == \"tanh\":\r\n","                    self.activation_gradients[i-1] = value * (1-np.square(self.activations[i]))\r\n","                elif self.activation_func == \"relu\":\r\n","                    res = []\r\n","                    for k in self.activations[i]:\r\n","                        if k>0: res.append(1.0)\r\n","                        else: res.append(0.0)\r\n","                    res = np.asarray(res)\r\n","                    self.activation_gradients[i-1] = value * res\r\n","\r\n","    def gradient_descent(self,x_train,y_train):\r\n","        for i in range(self.epochs):\r\n","            print('Epoch---',i+1,end=\" \")\r\n","            loss = 0\r\n","\r\n","            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","            \r\n","            index = 1\r\n","            for x,y in zip(x_train,y_train):\r\n","                x = x.ravel()\r\n","                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n","                self.backward_propagation(x,y,self.weights,self.biases)\r\n","\r\n","                if index % self.batch == 0 or index == x_train.shape[0]:\r\n","                    for j in range(len(self.weights)):\r\n","                        self.weights[j] -= self.learning_rate * self.weights_gradients[j]\r\n","                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\r\n","                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","                index += 1 \r\n","\r\n","            for x,y in zip(self.x_cv,self.y_cv):\r\n","               x=x.ravel()\r\n","               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n","\r\n","            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n","            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n","            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n","            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n","\r\n","\r\n","    def sgd(self,x_train,y_train):\r\n","        for i in range(self.epochs):\r\n","            print('Epoch---',i+1,end=\" \")\r\n","            loss = 0\r\n","            val_loss=0\r\n","\r\n","            index = 1\r\n","            for x,y in zip(x_train,y_train):\r\n","                x = x.ravel()\r\n","                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n","                self.backward_propagation(x,y,self.weights,self.biases)\r\n","\r\n","                if index % self.batch == 0 or index == x_train.shape[0]:\r\n","                    for j in range(len(self.weights)):\r\n","                        self.weights[j] -= self.learning_rate * self.weights_gradients[j]\r\n","                        self.biases[j] -= self.learning_rate * self.biases_gradients[j]\r\n","                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","                index +=1\r\n","\r\n","            for x,y in zip(self.x_cv,self.y_cv):\r\n","               x=x.ravel()\r\n","               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n","\r\n","            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n","            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n","            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n","            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n","\r\n","    \r\n","    def momentum(self,x_train,y_train):\r\n","        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n","        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n","\r\n","        for i in range(self.epochs):\r\n","            print('Epoch---',i+1,end=\" \")\r\n","            loss = 0\r\n","            val_loss=0\r\n","\r\n","            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","\r\n","            index = 1\r\n","            for x,y in zip(x_train,y_train):\r\n","                x = x.ravel()\r\n","                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n","                self.backward_propagation(x,y,self.weights,self.biases)\r\n","                if index % self.batch == 0 or index == x_train.shape[0]:\r\n","                    for j in range(len(self.weights)):\r\n","                        v_w = (self.decay_rate * prev_gradients_w[j] + self.learning_rate * self.weights_gradients[j])\r\n","                        v_b = (self.decay_rate * prev_gradients_b[j] + self.learning_rate * self.biases_gradients[j])\r\n","                        self.weights[j] -= v_w\r\n","                        self.biases[j] -= v_b\r\n","                        prev_gradients_w[j] = v_w\r\n","                        prev_gradients_b[j] = v_b\r\n","                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","\r\n","                index +=1\r\n","            for x,y in zip(self.x_cv,self.y_cv):\r\n","               x=x.ravel()\r\n","               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n","\r\n","            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n","            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n","            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n","            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n","\r\n","\r\n","    def nesterov(self,x_train,y_train):\r\n","        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n","        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n","\r\n","        for i in range(self.epochs):\r\n","            print('Epoch---',i+1,end=\" \")\r\n","            loss = 0\r\n","            val_loss=0\r\n","\r\n","            weights = [self.weights[j] -  self.decay_rate * prev_gradients_w[j] for j in range(len(self.weights))]\r\n","            biases = [self.biases[j] -  self.decay_rate * prev_gradients_b[j] for j in range(len(self.biases))]\r\n","\r\n","            self.weights_gradients = [0*j for j in (self.weights_gradients)]\r\n","            self.biases_gradients = [0*j for j in (self.biases_gradients)]\r\n","            index = 1\r\n","            for x,y in zip(x_train,y_train):\r\n","                x = x.ravel()\r\n","                loss += self.forward_propagation(x,y,weights,biases)\r\n","                self.backward_propagation(x,y,weights,biases)\r\n","                if index % self.batch == 0 or index == x_train.shape[0]:\r\n","                    for j in range(len(self.weights)):\r\n","                        prev_gradients_w[j] = self.decay_rate * prev_gradients_w[j] + self.learning_rate*self.weights_gradients[j] \r\n","                                           \r\n","                        prev_gradients_b[j] = self.decay_rate * prev_gradients_b[j] + self.learning_rate*self.biases_gradients[j] \r\n","                                        \r\n","                        self.weights[j] -= prev_gradients_w[j]\r\n","                        self.biases[j] -= prev_gradients_b[j]\r\n","\r\n","                    weights = [self.weights[j] -  self.decay_rate * prev_gradients_w[j] for j in range(len(self.weights))]\r\n","                    biases = [self.biases[j] -  self.decay_rate * prev_gradients_b[j] for j in range(len(self.biases))]\r\n","\r\n","                    self.weights_gradients = [0*j for j in (self.weights_gradients)]\r\n","                    self.biases_gradients = [0*j for j in (self.biases_gradients)]\r\n","                    \r\n","                index += 1\r\n","            for x,y in zip(self.x_cv,self.y_cv):\r\n","               x=x.ravel()\r\n","               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n","\r\n","            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n","            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n","            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n","            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n","\r\n","\r\n","    def rmsprop(self,x_train,y_train):\r\n","        prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n","        prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n","        eps = 1e-2\r\n","\r\n","        for i in range(self.epochs):\r\n","            print('Epoch---',i+1,end=\" \")\r\n","            loss = 0\r\n","            val_loss=0\r\n","\r\n","            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","\r\n","            index = 1\r\n","            for x,y in zip(x_train,y_train):\r\n","                x = x.ravel()\r\n","                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n","                self.backward_propagation(x,y,self.weights,self.biases)\r\n","                if index % self.batch == 0 or index == x_train.shape[0]:\r\n","                    for j in range(len(self.weights)):\r\n","                        v_w = (self.beta * prev_gradients_w[j] + (1-self.beta) * np.square(self.weights_gradients[j]))\r\n","                        v_b = (self.beta * prev_gradients_b[j] + (1-self.beta) * np.square(self.biases_gradients[j]))\r\n","                        self.weights[j] -= self.learning_rate * (self.weights_gradients[j] /(np.sqrt(v_w + eps)))\r\n","                        self.biases[j] -= self.learning_rate * (self.biases_gradients[j] /(np.sqrt(v_b + eps)))\r\n","                        prev_gradients_w[j] = v_w\r\n","                        prev_gradients_b[j] = v_b\r\n","                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","\r\n","                index +=1\r\n","            for x,y in zip(self.x_cv,self.y_cv):\r\n","               x=x.ravel()\r\n","               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n","\r\n","            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n","            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n","            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n","            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n","\r\n","\r\n","    def adam(self,x_train,y_train):\r\n","        m_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n","        m_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n","        v_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n","        v_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n","\r\n","        iter = 1\r\n","\r\n","        for i in range(self.epochs):\r\n","            print('Epoch---',i+1,end=\" \")\r\n","            loss = 0\r\n","            val_loss=0\r\n","            eps = 1e-2\r\n","            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","\r\n","            index = 1\r\n","            for x,y in zip(x_train,y_train):\r\n","                x = x.ravel()\r\n","                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n","                self.backward_propagation(x,y,self.weights,self.biases)\r\n","                if index % self.batch == 0 or index == x_train.shape[0]:\r\n","                    for j in range(len(self.weights)):\r\n","                        m_w = (self.beta1 * m_prev_gradients_w[j] + (1-self.beta1) * self.weights_gradients[j])\r\n","                        m_b = (self.beta1 * m_prev_gradients_b[j] + (1-self.beta1) * self.biases_gradients[j])\r\n","                        v_w = (self.beta2 * v_prev_gradients_w[j] + (1-self.beta2) * np.square(self.weights_gradients[j]))\r\n","                        v_b = (self.beta2 * v_prev_gradients_b[j] + (1-self.beta2) * np.square(self.biases_gradients[j]))\r\n","\r\n","                        m_hat_w = (m_w)/(1-(self.beta1)**iter) \r\n","                        m_hat_b = (m_b)/(1-(self.beta1)**iter) \r\n","\r\n","                        v_hat_w = (v_w)/(1-(self.beta2)**iter) \r\n","                        v_hat_b = (v_b)/(1-(self.beta2)**iter)\r\n","\r\n","                        self.weights[j] -= self.learning_rate * (m_hat_w/(np.sqrt(v_hat_w + eps)))\r\n","                        self.biases[j] -= self.learning_rate * (m_hat_b/(np.sqrt(v_hat_b + eps)))\r\n","\r\n","                        m_prev_gradients_w[j] = m_w\r\n","                        m_prev_gradients_b[j] = m_b\r\n","                        v_prev_gradients_w[j] = v_w\r\n","                        v_prev_gradients_b[j] = v_b\r\n","\r\n","                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","                    iter += 1\r\n","\r\n","                index +=1\r\n","            for x,y in zip(self.x_cv,self.y_cv):\r\n","               x=x.ravel()\r\n","               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n","\r\n","            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n","            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n","            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n","            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n","\r\n","    def nadam(self,x_train,y_train):\r\n","        m_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n","        m_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n","        v_prev_gradients_w = [0*i for i in (self.weights_gradients)]\r\n","        v_prev_gradients_b = [0*i for i in (self.biases_gradients)]\r\n","\r\n","        iter = 1\r\n","\r\n","        for i in range(self.epochs):\r\n","            print('Epoch---',i+1,end=\" \")\r\n","            loss = 0\r\n","            val_loss=0\r\n","            eps = 1e-2\r\n","            self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","            self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","\r\n","            index = 1\r\n","            for x,y in zip(x_train,y_train):\r\n","                x = x.ravel()\r\n","                loss += self.forward_propagation(x,y,self.weights,self.biases)\r\n","                self.backward_propagation(x,y,self.weights,self.biases)\r\n","                if index % self.batch == 0 or index == x_train.shape[0]:\r\n","                    for j in range(len(self.weights)):\r\n","\r\n","                        m_w = (self.beta1 * m_prev_gradients_w[j] + (1-self.beta1) * self.weights_gradients[j])\r\n","                        m_b = (self.beta1 * m_prev_gradients_b[j] + (1-self.beta1) * self.biases_gradients[j])\r\n","                        v_w = (self.beta2 * v_prev_gradients_w[j] + (1-self.beta2) * np.square(self.weights_gradients[j]))\r\n","                        v_b = (self.beta2 * v_prev_gradients_b[j] + (1-self.beta2) * np.square(self.biases_gradients[j]))\r\n","\r\n","                        m_hat_w = (m_w)/(1-(self.beta1)**iter) \r\n","                        m_hat_b = (m_b)/(1-(self.beta1)**iter) \r\n","\r\n","                        v_hat_w = (v_w)/(1-(self.beta2)**iter) \r\n","                        v_hat_b = (v_b)/(1-(self.beta2)**iter)\r\n","\r\n","                        m_dash_w = self.beta1 * m_hat_w + (1-self.beta1) * self.weights_gradients[j]\r\n","                        m_dash_b = self.beta1 * m_hat_b + (1-self.beta1) * self.biases_gradients[j]\r\n","\r\n","                        self.weights[j] -= self.learning_rate * (m_dash_w/(np.sqrt(v_hat_w + eps)))\r\n","                        self.biases[j] -= self.learning_rate * (m_dash_b/(np.sqrt(v_hat_b + eps)))\r\n","\r\n","                        m_prev_gradients_w[j] = m_w\r\n","                        m_prev_gradients_b[j] = m_b\r\n","                        v_prev_gradients_w[j] = v_w\r\n","                        v_prev_gradients_b[j] = v_b\r\n","\r\n","                    self.weights_gradients = [0*i for i in (self.weights_gradients)]\r\n","                    self.biases_gradients = [0*i for i in (self.biases_gradients)]\r\n","                    iter += 1\r\n","\r\n","                index +=1\r\n","\r\n","            for x,y in zip(self.x_cv,self.y_cv):\r\n","               x=x.ravel()\r\n","               val_loss+=self.forward_propagation(x,y,self.weights,self.biases)\r\n","\r\n","            acc=round(self.calculate_accuracy(x_train,y_train),3)\r\n","            val_acc=round(self.calculate_accuracy(self.x_cv,self.y_cv),3)\r\n","            # wandb.log({'train_loss':loss/x_train.shape[0],'train_accuracy':acc,'val_loss':val_loss/self.x_cv.shape[0],'val_accuarcy':val_acc})\r\n","            print('  loss = ',loss/x_train.shape[0],'  accuracy = ',acc,'   validation loss= ',val_loss/self.x_cv.shape[0],'  validation accuaracy= ',val_acc)\r\n","\r\n","\r\n","    def calculate_accuracy(self,X,Y):\r\n","        count = 0\r\n","        for i in range(len(X)):\r\n","            if self.predict(X[i]) == Y[i]:\r\n","                count+=1\r\n","        return count/len(X)\r\n","\r\n","\r\n","    def predict(self,x):\r\n","        x = x.ravel()\r\n","        self.activations[0] = x\r\n","        n = len(self.layers)\r\n","        for i in range(n-2):\r\n","            if self.activation_func == \"sigmoid\":\r\n","                self.activations[i+1] = self.sigmoid(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n","            elif self.activation_func == \"tanh\":\r\n","                self.activations[i+1] = self.tanh(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n","            elif self.activation_func == \"relu\":\r\n","                self.activations[i+1] = self.relu(np.matmul(self.weights[i].T,self.activations[i])+self.biases[i])\r\n","\r\n","        self.activations[n-1] = self.softmax(np.matmul(self.weights[n-2].T,self.activations[n-2])+self.biases[n-2])\r\n","\r\n","        return np.argmax(self.activations[-1])"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UXRutAAeL6fz","executionInfo":{"status":"ok","timestamp":1615618798673,"user_tz":-330,"elapsed":672921,"user":{"displayName":"Aditya Kumar cs20m009","photoUrl":"https://lh5.googleusercontent.com/-tW0zCvBKMxY/AAAAAAAAAAI/AAAAAAAAALI/pbX7svHnMjQ/s64/photo.jpg","userId":"13560681502679086256"}},"outputId":"3aeb0a1b-fe10-417e-b0fe-cf5d8d634663"},"source":["nn = Neural_network(x_train,y_train,784,32,2,10,learning_rate=0.0001,batch_size = 256,epochs=3,\r\n","                    activation_func=\"tanh\",optimizer=\"adam\",weight_init=\"random\",decay_rate=0.8,loss=\"cross_entropy\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch--- 1   loss =  4.410875810387893   accuracy =  0.285    validation loss=  3.267206881955507   validation accuaracy=  0.289\n","Epoch--- 2   loss =  2.6746176304116274   accuracy =  0.449    validation loss=  2.3424720007002713   validation accuaracy=  0.443\n","Epoch--- 3   loss =  2.0974903848117274   accuracy =  0.539    validation loss=  1.9622489860578056   validation accuaracy=  0.532\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ePFgCv1L1hc","executionInfo":{"status":"ok","timestamp":1615618473359,"user_tz":-330,"elapsed":351282,"user":{"displayName":"Aditya Kumar cs20m009","photoUrl":"https://lh5.googleusercontent.com/-tW0zCvBKMxY/AAAAAAAAAAI/AAAAAAAAALI/pbX7svHnMjQ/s64/photo.jpg","userId":"13560681502679086256"}},"outputId":"51dc3c99-7be2-4027-fc39-98288a00fada"},"source":["nn = Neural_network(x_train,y_train,784,32,2,10,learning_rate=0.0001,batch_size = 256,epochs=3,\r\n","                    activation_func=\"tanh\",optimizer=\"adam\",weight_init=\"random\",decay_rate=0.8,loss=\"squared_loss\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch--- 1   loss =  1.0322509063777892   accuracy =  0.331    validation loss=  0.8902181314833233   validation accuaracy=  0.333\n","Epoch--- 2   loss =  0.8218366202268449   accuracy =  0.422    validation loss=  0.7631890619922613   validation accuaracy=  0.414\n","Epoch--- 3   loss =  0.7080123265647982   accuracy =  0.491    validation loss=  0.6725003234893882   validation accuaracy=  0.482\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qTl89ow89KwW"},"source":["**Here in the above two cells, in the first cell we computed with loss as cross entropy for 3 epochs then we get validation accuracy of 53.2% and for the same configuration if we use loss as squared loss then we get accuracy of 48.2%**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zSkak8-NY_an","executionInfo":{"status":"ok","timestamp":1615619821191,"user_tz":-330,"elapsed":258997,"user":{"displayName":"Aditya Kumar cs20m009","photoUrl":"https://lh5.googleusercontent.com/-tW0zCvBKMxY/AAAAAAAAAAI/AAAAAAAAALI/pbX7svHnMjQ/s64/photo.jpg","userId":"13560681502679086256"}},"outputId":"29af17bf-1098-4c93-a41d-99b92e684957"},"source":["nn = Neural_network(x_train,y_train,784,16,3,10,learning_rate=0.0001,batch_size = 64,epochs=3,\r\n","                    activation_func=\"tanh\",optimizer=\"nadam\",weight_init=\"xavier\",decay_rate=0.8,loss=\"cross_entropy\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch--- 1   loss =  2.840050576380326   accuracy =  0.315    validation loss=  2.497627981540466   validation accuaracy=  0.311\n","Epoch--- 2   loss =  2.323451287621075   accuracy =  0.535    validation loss=  2.1520652586860063   validation accuaracy=  0.529\n","Epoch--- 3   loss =  1.9680381998060257   accuracy =  0.549    validation loss=  1.8275632717325387   validation accuaracy=  0.546\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qPWw1bK-Y-gr","executionInfo":{"status":"ok","timestamp":1615619283008,"user_tz":-330,"elapsed":294251,"user":{"displayName":"Aditya Kumar cs20m009","photoUrl":"https://lh5.googleusercontent.com/-tW0zCvBKMxY/AAAAAAAAAAI/AAAAAAAAALI/pbX7svHnMjQ/s64/photo.jpg","userId":"13560681502679086256"}},"outputId":"1a43b828-7427-49ab-e753-c1f0528d0974"},"source":["nn = Neural_network(x_train,y_train,784,16,3,10,learning_rate=0.0001,batch_size = 64,epochs=3,\r\n","                    activation_func=\"tanh\",optimizer=\"nadam\",weight_init=\"xavier\",decay_rate=0.8,loss=\"squared_loss\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch--- 1   loss =  0.862686059173855   accuracy =  0.289    validation loss=  0.8153129325006884   validation accuaracy=  0.287\n","Epoch--- 2   loss =  0.778524200603818   accuracy =  0.438    validation loss=  0.7393067923006332   validation accuaracy=  0.428\n","Epoch--- 3   loss =  0.6907123551506595   accuracy =  0.529    validation loss=  0.646048442587927   validation accuaracy=  0.529\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"18cNlteH9DDh"},"source":["**Here in the above two cells, in the first cell we computed with loss as cross entropy for 3 epochs then we get validation accuracy of 54.6% and for the same configuration if we use loss as squared loss then we get accuracy of 52.9%**"]},{"cell_type":"code","metadata":{"id":"XbM2LUOf6Wq1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615620619189,"user_tz":-330,"elapsed":845,"user":{"displayName":"Varun Sai Dakavaram cs20m069","photoUrl":"","userId":"01521556991466858861"}},"outputId":"3c88c692-5d4b-4405-f329-573b06c0fb7a"},"source":["nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=3e-3,batch_size = 64,epochs=3,\r\n","                    activation_func=\"relu\",optimizer=\"adam\",weight_init=\"xavier\",decay_rate=0.9,loss=\"cross_entropy\")"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Epoch--- 1   loss =  0.872479811689399   accuracy =  0.826    validation loss=  0.6899052297527019   validation accuaracy=  0.816\n","Epoch--- 2   loss =  0.5919632520375588   accuracy =  0.857    validation loss=  0.5877115601479391   validation accuaracy=  0.847\n","Epoch--- 3   loss =  0.5355915003189532   accuracy =  0.868    validation loss=  0.546800380734861   validation accuaracy=  0.862\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vlsUxvm9JGzY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615620594355,"user_tz":-330,"elapsed":879,"user":{"displayName":"Varun Sai Dakavaram cs20m069","photoUrl":"","userId":"01521556991466858861"}},"outputId":"4d7c4a63-d93c-42c4-9029-1442e00824a7"},"source":["nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=3e-3,batch_size = 64,epochs=3,\r\n","                    activation_func=\"relu\",optimizer=\"adam\",weight_init=\"xavier\",decay_rate=0.9,loss=\"squared_loss\")"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Epoch--- 1   loss =  0.3264697301892217   accuracy =  0.839    validation loss=  0.24180666089717034   validation accuaracy=  0.829\n","Epoch--- 2   loss =  0.21795579690597414   accuracy =  0.85    validation loss=  0.22580669811185336   validation accuaracy=  0.841\n","Epoch--- 3   loss =  0.19817767856436266   accuracy =  0.839    validation loss=  0.24834383598369933   validation accuaracy=  0.825\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1r9_OmcA81So"},"source":["**bold Here in the above two cells, in the first cell we computed with loss as cross entropy for 3 epochs then we get validation accuracy of 86.2% and for the same configuration if we use loss as squared loss then we get accuracy of 82.5%text**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYTss5DtItY_","executionInfo":{"status":"ok","timestamp":1615622814576,"user_tz":-330,"elapsed":984,"user":{"displayName":"Varun Sai Dakavaram cs20m069","photoUrl":"","userId":"01521556991466858861"}},"outputId":"ebc8ba41-a1f0-4d00-f81d-27c0573a33d9"},"source":["nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=2e-3,batch_size= 64,epochs=6,\r\n","                    activation_func=\"tanh\",optimizer=\"nadam\",weight_init=\"xavier\",decay_rate=0.1,loss=\"cross_entropy\")"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Epoch--- 1   loss =  0.8026560179978357   accuracy =  0.856    validation loss=  0.5974650405768595   validation accuaracy=  0.842\n","Epoch--- 2   loss =  0.5472850285954772   accuracy =  0.869    validation loss=  0.5443255065183352   validation accuaracy=  0.855\n","Epoch--- 3   loss =  0.4970527330993899   accuracy =  0.875    validation loss=  0.5257676483428697   validation accuaracy=  0.86\n","Epoch--- 4   loss =  0.465177442865648   accuracy =  0.883    validation loss=  0.5032145444455606   validation accuaracy=  0.867\n","Epoch--- 5   loss =  0.44067019984154865   accuracy =  0.887    validation loss=  0.48671335359921153   validation accuaracy=  0.875\n","Epoch--- 6   loss =  0.4213724798032104   accuracy =  0.891    validation loss=  0.4873435136591763   validation accuaracy=  0.877\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dLnrmRsCiq4I","executionInfo":{"status":"ok","timestamp":1615622840585,"user_tz":-330,"elapsed":931,"user":{"displayName":"Varun Sai Dakavaram cs20m069","photoUrl":"","userId":"01521556991466858861"}},"outputId":"4c515395-da97-4123-8447-8776b4f1afb1"},"source":["nn = Neural_network(x_train,y_train,784,64,2,10,learning_rate=2e-3,batch_size= 64,epochs=6,\r\n","                    activation_func=\"tanh\",optimizer=\"nadam\",weight_init=\"xavier\",decay_rate=0.1,loss=\"squared_loss\")"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Epoch--- 1   loss =  0.29698845337618457   accuracy =  0.851    validation loss=  0.22552089670677541   validation accuaracy=  0.838\n","Epoch--- 2   loss =  0.204455888607373   accuracy =  0.857    validation loss=  0.217656674040274   validation accuaracy=  0.844\n","Epoch--- 3   loss =  0.1868245100858147   accuracy =  0.863    validation loss=  0.2147361265814986   validation accuaracy=  0.847\n","Epoch--- 4   loss =  0.17644376709351361   accuracy =  0.868    validation loss=  0.20919306435838086   validation accuaracy=  0.855\n","Epoch--- 5   loss =  0.16864282594379246   accuracy =  0.873    validation loss=  0.20128879951126222   validation accuaracy=  0.859\n","Epoch--- 6   loss =  0.16265461932845374   accuracy =  0.874    validation loss=  0.20360691831269295   validation accuaracy=  0.856\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eDXtAA2k8VQo"},"source":["**Here in the above two cells, in the first cell we computed with loss as cross entropy for 6 epochs then we get validation accuracy of 87.7% and for the same configuration if we use loss as squared loss then we get accuracy of 85.6%**"]}]}